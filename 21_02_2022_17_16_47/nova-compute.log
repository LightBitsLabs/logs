Build Started 21_02_2022_17_16_47
2022-02-21 19:18:50.835 7 INFO nova.virt.libvirt.imagecache [req-7d947122-5cdd-4be4-befb-5d865e596a1d - - - - -] image 54b95c78-b439-4b0b-8e00-3a7cc508dc31 at (/var/lib/nova/instances/_base/f26e4543eb5697d89d40a430f4bf0ab5380c1b8b): checking
2022-02-21 19:18:50.997 7 INFO nova.virt.libvirt.imagecache [req-7d947122-5cdd-4be4-befb-5d865e596a1d - - - - -] Active base files: /var/lib/nova/instances/_base/f26e4543eb5697d89d40a430f4bf0ab5380c1b8b
2022-02-21 19:19:29.077 7 WARNING amqp [-] Received method (60, 30) during closing channel 1. This method will be ignored
2022-02-21 19:19:33.122 7 INFO os_vif [-] Loaded VIF plugins: linux_bridge, noop, ovs
2022-02-21 19:19:34.050 7 INFO nova.virt.driver [req-08ad2b5b-b41e-4523-98ec-6ce62f4789bb - - - - -] Loading compute driver 'libvirt.LibvirtDriver'
2022-02-21 19:19:34.441 7 INFO nova.compute.provider_config [req-08ad2b5b-b41e-4523-98ec-6ce62f4789bb - - - - -] No provider configs found in /etc/nova/provider_config/. If files are present, ensure the Nova process has access.
2022-02-21 19:19:34.460 7 WARNING oslo_config.cfg [req-08ad2b5b-b41e-4523-98ec-6ce62f4789bb - - - - -] Deprecated: Option "api_servers" from group "glance" is deprecated for removal (
Support for image service configuration via standard keystoneauth1 Adapter
options was added in the 17.0.0 Queens release. The api_servers option was
retained temporarily to allow consumers time to cut over to a real load
balancing solution.
).  Its value may be silently ignored in the future.
2022-02-21 19:19:34.482 7 INFO nova.service [-] Starting compute node (version 24.1.0)
2022-02-21 19:19:34.496 7 INFO nova.virt.libvirt.driver [-] Connection event '1' reason 'None'
2022-02-21 19:19:34.532 7 INFO nova.virt.libvirt.host [-] Libvirt host capabilities <capabilities>

  <host>
    <uuid>00d8c757-53fc-e811-906e-0012795d9712</uuid>
    <cpu>
      <arch>x86_64</arch>
      <model>Broadwell-IBRS</model>
      <vendor>Intel</vendor>
      <microcode version='184549438'/>
      <counter name='tsc' frequency='2095147000' scaling='no'/>
      <topology sockets='1' cores='8' threads='2'/>
      <feature name='vme'/>
      <feature name='ds'/>
      <feature name='acpi'/>
      <feature name='ss'/>
      <feature name='ht'/>
      <feature name='tm'/>
      <feature name='pbe'/>
      <feature name='dtes64'/>
      <feature name='monitor'/>
      <feature name='ds_cpl'/>
      <feature name='vmx'/>
      <feature name='smx'/>
      <feature name='est'/>
      <feature name='tm2'/>
      <feature name='xtpr'/>
      <feature name='pdcm'/>
      <feature name='dca'/>
      <feature name='osxsave'/>
      <feature name='f16c'/>
      <feature name='rdrand'/>
      <feature name='arat'/>
      <feature name='tsc_adjust'/>
      <feature name='cmt'/>
      <feature name='intel-pt'/>
      <feature name='md-clear'/>
      <feature name='stibp'/>
      <feature name='ssbd'/>
      <feature name='xsaveopt'/>
      <feature name='mbm_total'/>
      <feature name='mbm_local'/>
      <feature name='pdpe1gb'/>
      <feature name='abm'/>
      <feature name='invtsc'/>
      <pages unit='KiB' size='4'/>
      <pages unit='KiB' size='2048'/>
      <pages unit='KiB' size='1048576'/>
    </cpu>
    <power_management>
      <suspend_mem/>
      <suspend_disk/>
      <suspend_hybrid/>
    </power_management>
    <iommu support='yes'/>
    <migration_features>
      <live/>
      <uri_transports>
        <uri_transport>tcp</uri_transport>
        <uri_transport>rdma</uri_transport>
      </uri_transports>
    </migration_features>
    <topology>
      <cells num='2'>
        <cell id='0'>
          <memory unit='KiB'>32772584</memory>
          <pages unit='KiB' size='4'>8193146</pages>
          <pages unit='KiB' size='2048'>0</pages>
          <pages unit='KiB' size='1048576'>0</pages>
          <distances>
            <sibling id='0' value='10'/>
            <sibling id='1' value='21'/>
          </distances>
          <cpus num='16'>
            <cpu id='0' socket_id='0' core_id='0' siblings='0,16'/>
            <cpu id='1' socket_id='0' core_id='1' siblings='1,17'/>
            <cpu id='2' socket_id='0' core_id='2' siblings='2,18'/>
            <cpu id='3' socket_id='0' core_id='3' siblings='3,19'/>
            <cpu id='4' socket_id='0' core_id='4' siblings='4,20'/>
            <cpu id='5' socket_id='0' core_id='5' siblings='5,21'/>
            <cpu id='6' socket_id='0' core_id='6' siblings='6,22'/>
            <cpu id='7' socket_id='0' core_id='7' siblings='7,23'/>
            <cpu id='16' socket_id='0' core_id='0' siblings='0,16'/>
            <cpu id='17' socket_id='0' core_id='1' siblings='1,17'/>
            <cpu id='18' socket_id='0' core_id='2' siblings='2,18'/>
            <cpu id='19' socket_id='0' core_id='3' siblings='3,19'/>
            <cpu id='20' socket_id='0' core_id='4' siblings='4,20'/>
            <cpu id='21' socket_id='0' core_id='5' siblings='5,21'/>
            <cpu id='22' socket_id='0' core_id='6' siblings='6,22'/>
            <cpu id='23' socket_id='0' core_id='7' siblings='7,23'/>
          </cpus>
        </cell>
        <cell id='1'>
          <memory unit='KiB'>16511320</memory>
          <pages unit='KiB' size='4'>4127830</pages>
          <pages unit='KiB' size='2048'>0</pages>
          <pages unit='KiB' size='1048576'>0</pages>
          <distances>
            <sibling id='0' value='21'/>
            <sibling id='1' value='10'/>
          </distances>
          <cpus num='16'>
            <cpu id='8' socket_id='1' core_id='0' siblings='8,24'/>
            <cpu id='9' socket_id='1' core_id='1' siblings='9,25'/>
            <cpu id='10' socket_id='1' core_id='2' siblings='10,26'/>
            <cpu id='11' socket_id='1' core_id='3' siblings='11,27'/>
            <cpu id='12' socket_id='1' core_id='4' siblings='12,28'/>
            <cpu id='13' socket_id='1' core_id='5' siblings='13,29'/>
            <cpu id='14' socket_id='1' core_id='6' siblings='14,30'/>
            <cpu id='15' socket_id='1' core_id='7' siblings='15,31'/>
            <cpu id='24' socket_id='1' core_id='0' siblings='8,24'/>
            <cpu id='25' socket_id='1' core_id='1' siblings='9,25'/>
            <cpu id='26' socket_id='1' core_id='2' siblings='10,26'/>
            <cpu id='27' socket_id='1' core_id='3' siblings='11,27'/>
            <cpu id='28' socket_id='1' core_id='4' siblings='12,28'/>
            <cpu id='29' socket_id='1' core_id='5' siblings='13,29'/>
            <cpu id='30' socket_id='1' core_id='6' siblings='14,30'/>
            <cpu id='31' socket_id='1' core_id='7' siblings='15,31'/>
          </cpus>
        </cell>
      </cells>
    </topology>
    <cache>
      <bank id='0' level='3' type='both' size='20' unit='MiB' cpus='0-7,16-23'/>
      <bank id='1' level='3' type='both' size='20' unit='MiB' cpus='8-15,24-31'/>
    </cache>
    <secmodel>
      <model>none</model>
      <doi>0</doi>
    </secmodel>
    <secmodel>
      <model>dac</model>
      <doi>0</doi>
      <baselabel type='kvm'>+42436:+42436</baselabel>
      <baselabel type='qemu'>+42436:+42436</baselabel>
    </secmodel>
  </host>

  <guest>
    <os_type>hvm</os_type>
    <arch name='alpha'>
      <wordsize>64</wordsize>
      <emulator>/usr/bin/qemu-system-alpha</emulator>
      <machine maxCpus='4'>clipper</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='armv6l'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-arm</emulator>
      <machine maxCpus='1'>integratorcp</machine>
      <machine maxCpus='1'>versatileab</machine>
      <machine maxCpus='2'>smdkc210</machine>
      <machine maxCpus='2'>nuri</machine>
      <machine maxCpus='2'>palmetto-bmc</machine>
      <machine maxCpus='1'>microbit</machine>
      <machine maxCpus='1'>mainstone</machine>
      <machine maxCpus='1'>canon-a1100</machine>
      <machine maxCpus='512'>virt-4.2</machine>
      <machine canonical='virt-4.2' maxCpus='512'>virt</machine>
      <machine maxCpus='1'>terrier</machine>
      <machine maxCpus='1'>n800</machine>
      <machine maxCpus='1'>verdex</machine>
      <machine maxCpus='1'>kzm</machine>
      <machine maxCpus='1'>versatilepb</machine>
      <machine maxCpus='4'>midway</machine>
      <machine maxCpus='255'>virt-2.7</machine>
      <machine maxCpus='1'>emcraft-sf2</machine>
      <machine maxCpus='2'>mcimx7d-sabre</machine>
      <machine maxCpus='4'>highbank</machine>
      <machine maxCpus='1'>imx25-pdk</machine>
      <machine maxCpus='255'>virt-2.11</machine>
      <machine maxCpus='1'>n810</machine>
      <machine maxCpus='4'>realview-pbx-a9</machine>
      <machine maxCpus='1'>tosa</machine>
      <machine maxCpus='1'>sx1-v1</machine>
      <machine maxCpus='2'>swift-bmc</machine>
      <machine maxCpus='1'>mps2-an511</machine>
      <machine maxCpus='4'>sabrelite</machine>
      <machine maxCpus='2'>musca-a</machine>
      <machine maxCpus='1'>realview-pb-a8</machine>
      <machine maxCpus='1'>akita</machine>
      <machine maxCpus='1'>cubieboard</machine>
      <machine maxCpus='255'>virt-2.9</machine>
      <machine maxCpus='1'>xilinx-zynq-a9</machine>
      <machine maxCpus='512'>virt-3.1</machine>
      <machine maxCpus='4'>vexpress-a15</machine>
      <machine maxCpus='512'>virt-4.1</machine>
      <machine maxCpus='2'>musca-b1</machine>
      <machine maxCpus='2'>mps2-an521</machine>
      <machine maxCpus='1'>realview-eb</machine>
      <machine maxCpus='2'>ast2600-evb</machine>
      <machine maxCpus='1'>cheetah</machine>
      <machine maxCpus='4'>vexpress-a9</machine>
      <machine maxCpus='255'>virt-2.6</machine>
      <machine maxCpus='1'>mps2-an505</machine>
      <machine maxCpus='1'>spitz</machine>
      <machine maxCpus='1'>netduino2</machine>
      <machine maxCpus='1'>lm3s6965evb</machine>
      <machine maxCpus='255'>virt-2.10</machine>
      <machine maxCpus='1'>lm3s811evb</machine>
      <machine maxCpus='1'>z2</machine>
      <machine maxCpus='4'>raspi2</machine>
      <machine maxCpus='1'>connex</machine>
      <machine maxCpus='1'>borzoi</machine>
      <machine maxCpus='1'>mcimx6ul-evk</machine>
      <machine maxCpus='1'>musicpal</machine>
      <machine maxCpus='255'>virt-2.8</machine>
      <machine maxCpus='2'>ast2500-evb</machine>
      <machine maxCpus='512'>virt-3.0</machine>
      <machine maxCpus='1'>sx1</machine>
      <machine maxCpus='4'>realview-eb-mpcore</machine>
      <machine maxCpus='2'>witherspoon-bmc</machine>
      <machine maxCpus='2'>romulus-bmc</machine>
      <machine maxCpus='512'>virt-4.0</machine>
      <machine maxCpus='1'>mps2-an385</machine>
      <machine maxCpus='255'>virt-2.12</machine>
      <machine maxCpus='1'>collie</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='armv7l'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-arm</emulator>
      <machine maxCpus='1'>integratorcp</machine>
      <machine maxCpus='1'>versatileab</machine>
      <machine maxCpus='2'>smdkc210</machine>
      <machine maxCpus='2'>nuri</machine>
      <machine maxCpus='2'>palmetto-bmc</machine>
      <machine maxCpus='1'>microbit</machine>
      <machine maxCpus='1'>mainstone</machine>
      <machine maxCpus='1'>canon-a1100</machine>
      <machine maxCpus='512'>virt-4.2</machine>
      <machine canonical='virt-4.2' maxCpus='512'>virt</machine>
      <machine maxCpus='1'>terrier</machine>
      <machine maxCpus='1'>n800</machine>
      <machine maxCpus='1'>verdex</machine>
      <machine maxCpus='1'>kzm</machine>
      <machine maxCpus='1'>versatilepb</machine>
      <machine maxCpus='4'>midway</machine>
      <machine maxCpus='255'>virt-2.7</machine>
      <machine maxCpus='1'>emcraft-sf2</machine>
      <machine maxCpus='2'>mcimx7d-sabre</machine>
      <machine maxCpus='4'>highbank</machine>
      <machine maxCpus='1'>imx25-pdk</machine>
      <machine maxCpus='255'>virt-2.11</machine>
      <machine maxCpus='1'>n810</machine>
      <machine maxCpus='4'>realview-pbx-a9</machine>
      <machine maxCpus='1'>tosa</machine>
      <machine maxCpus='1'>sx1-v1</machine>
      <machine maxCpus='2'>swift-bmc</machine>
      <machine maxCpus='1'>mps2-an511</machine>
      <machine maxCpus='4'>sabrelite</machine>
      <machine maxCpus='2'>musca-a</machine>
      <machine maxCpus='1'>realview-pb-a8</machine>
      <machine maxCpus='1'>akita</machine>
      <machine maxCpus='1'>cubieboard</machine>
      <machine maxCpus='255'>virt-2.9</machine>
      <machine maxCpus='1'>xilinx-zynq-a9</machine>
      <machine maxCpus='512'>virt-3.1</machine>
      <machine maxCpus='4'>vexpress-a15</machine>
      <machine maxCpus='512'>virt-4.1</machine>
      <machine maxCpus='2'>musca-b1</machine>
      <machine maxCpus='2'>mps2-an521</machine>
      <machine maxCpus='1'>realview-eb</machine>
      <machine maxCpus='2'>ast2600-evb</machine>
      <machine maxCpus='1'>cheetah</machine>
      <machine maxCpus='4'>vexpress-a9</machine>
      <machine maxCpus='255'>virt-2.6</machine>
      <machine maxCpus='1'>mps2-an505</machine>
      <machine maxCpus='1'>spitz</machine>
      <machine maxCpus='1'>netduino2</machine>
      <machine maxCpus='1'>lm3s6965evb</machine>
      <machine maxCpus='255'>virt-2.10</machine>
      <machine maxCpus='1'>lm3s811evb</machine>
      <machine maxCpus='1'>z2</machine>
      <machine maxCpus='4'>raspi2</machine>
      <machine maxCpus='1'>connex</machine>
      <machine maxCpus='1'>borzoi</machine>
      <machine maxCpus='1'>mcimx6ul-evk</machine>
      <machine maxCpus='1'>musicpal</machine>
      <machine maxCpus='255'>virt-2.8</machine>
      <machine maxCpus='2'>ast2500-evb</machine>
      <machine maxCpus='512'>virt-3.0</machine>
      <machine maxCpus='1'>sx1</machine>
      <machine maxCpus='4'>realview-eb-mpcore</machine>
      <machine maxCpus='2'>witherspoon-bmc</machine>
      <machine maxCpus='2'>romulus-bmc</machine>
      <machine maxCpus='512'>virt-4.0</machine>
      <machine maxCpus='1'>mps2-an385</machine>
      <machine maxCpus='255'>virt-2.12</machine>
      <machine maxCpus='1'>collie</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='aarch64'>
      <wordsize>64</wordsize>
      <emulator>/usr/bin/qemu-system-aarch64</emulator>
      <machine maxCpus='1'>integratorcp</machine>
      <machine maxCpus='1'>versatileab</machine>
      <machine maxCpus='2'>xlnx-versal-virt</machine>
      <machine maxCpus='2'>smdkc210</machine>
      <machine maxCpus='2'>nuri</machine>
      <machine maxCpus='2'>palmetto-bmc</machine>
      <machine maxCpus='1'>microbit</machine>
      <machine maxCpus='512'>sbsa-ref</machine>
      <machine maxCpus='1'>mainstone</machine>
      <machine maxCpus='1'>canon-a1100</machine>
      <machine maxCpus='512'>virt-4.2</machine>
      <machine canonical='virt-4.2' maxCpus='512'>virt</machine>
      <machine maxCpus='1'>terrier</machine>
      <machine maxCpus='1'>n800</machine>
      <machine maxCpus='1'>verdex</machine>
      <machine maxCpus='1'>kzm</machine>
      <machine maxCpus='1'>versatilepb</machine>
      <machine maxCpus='4'>midway</machine>
      <machine maxCpus='255'>virt-2.7</machine>
      <machine maxCpus='1'>emcraft-sf2</machine>
      <machine maxCpus='2'>mcimx7d-sabre</machine>
      <machine maxCpus='4'>highbank</machine>
      <machine maxCpus='1'>imx25-pdk</machine>
      <machine maxCpus='1'>n810</machine>
      <machine maxCpus='255'>virt-2.11</machine>
      <machine maxCpus='4'>realview-pbx-a9</machine>
      <machine maxCpus='1'>tosa</machine>
      <machine maxCpus='1'>sx1-v1</machine>
      <machine maxCpus='2'>swift-bmc</machine>
      <machine maxCpus='4'>raspi3</machine>
      <machine maxCpus='1'>mps2-an511</machine>
      <machine maxCpus='4'>sabrelite</machine>
      <machine maxCpus='2'>musca-a</machine>
      <machine maxCpus='1'>realview-pb-a8</machine>
      <machine maxCpus='1'>akita</machine>
      <machine maxCpus='1'>cubieboard</machine>
      <machine maxCpus='255'>virt-2.9</machine>
      <machine maxCpus='1'>xilinx-zynq-a9</machine>
      <machine maxCpus='512'>virt-3.1</machine>
      <machine maxCpus='4'>vexpress-a15</machine>
      <machine maxCpus='512'>virt-4.1</machine>
      <machine maxCpus='2'>musca-b1</machine>
      <machine maxCpus='2'>mps2-an521</machine>
      <machine maxCpus='1'>realview-eb</machine>
      <machine maxCpus='2'>ast2600-evb</machine>
      <machine maxCpus='1'>cheetah</machine>
      <machine maxCpus='4'>vexpress-a9</machine>
      <machine maxCpus='255'>virt-2.6</machine>
      <machine maxCpus='1'>mps2-an505</machine>
      <machine maxCpus='1'>spitz</machine>
      <machine maxCpus='1'>netduino2</machine>
      <machine maxCpus='1'>lm3s6965evb</machine>
      <machine maxCpus='255'>virt-2.10</machine>
      <machine maxCpus='1'>lm3s811evb</machine>
      <machine maxCpus='6'>xlnx-zcu102</machine>
      <machine maxCpus='1'>z2</machine>
      <machine maxCpus='4'>raspi2</machine>
      <machine maxCpus='1'>connex</machine>
      <machine maxCpus='1'>borzoi</machine>
      <machine maxCpus='1'>mcimx6ul-evk</machine>
      <machine maxCpus='1'>musicpal</machine>
      <machine maxCpus='255'>virt-2.8</machine>
      <machine maxCpus='2'>ast2500-evb</machine>
      <machine maxCpus='512'>virt-3.0</machine>
      <machine maxCpus='1'>sx1</machine>
      <machine maxCpus='4'>realview-eb-mpcore</machine>
      <machine maxCpus='2'>witherspoon-bmc</machine>
      <machine maxCpus='2'>romulus-bmc</machine>
      <machine maxCpus='512'>virt-4.0</machine>
      <machine maxCpus='1'>mps2-an385</machine>
      <machine maxCpus='255'>virt-2.12</machine>
      <machine maxCpus='1'>collie</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <acpi default='on' toggle='yes'/>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='cris'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-cris</emulator>
      <machine maxCpus='1'>axis-dev88</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='i686'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-i386</emulator>
      <machine maxCpus='255'>pc-i440fx-focal</machine>
      <machine canonical='pc-i440fx-focal' maxCpus='255'>ubuntu</machine>
      <machine maxCpus='255'>pc-0.15</machine>
      <machine maxCpus='255'>pc-i440fx-2.12</machine>
      <machine maxCpus='255'>pc-i440fx-2.0</machine>
      <machine maxCpus='255'>pc-i440fx-xenial</machine>
      <machine maxCpus='288'>pc-q35-4.2</machine>
      <machine canonical='pc-q35-4.2' maxCpus='288'>q35</machine>
      <machine maxCpus='255'>pc-i440fx-2.5</machine>
      <machine maxCpus='255'>pc-i440fx-4.2</machine>
      <machine canonical='pc-i440fx-4.2' maxCpus='255'>pc</machine>
      <machine maxCpus='255'>pc-q35-xenial</machine>
      <machine maxCpus='255'>pc-i440fx-1.5</machine>
      <machine maxCpus='255'>pc-0.12</machine>
      <machine maxCpus='255'>pc-q35-2.7</machine>
      <machine maxCpus='288'>pc-q35-eoan-hpb</machine>
      <machine maxCpus='255'>pc-i440fx-disco-hpb</machine>
      <machine maxCpus='255'>pc-i440fx-zesty</machine>
      <machine maxCpus='288'>pc-q35-artful</machine>
      <machine maxCpus='255'>pc-i440fx-trusty</machine>
      <machine maxCpus='255'>pc-i440fx-2.2</machine>
      <machine maxCpus='255'>pc-i440fx-eoan-hpb</machine>
      <machine maxCpus='288'>pc-q35-focal-hpb</machine>
      <machine maxCpus='255'>pc-1.1</machine>
      <machine maxCpus='288'>pc-q35-bionic-hpb</machine>
      <machine maxCpus='255'>pc-i440fx-artful</machine>
      <machine maxCpus='255'>pc-i440fx-2.7</machine>
      <machine maxCpus='255'>pc-i440fx-yakkety</machine>
      <machine maxCpus='255'>pc-q35-2.4</machine>
      <machine maxCpus='288'>pc-q35-cosmic-hpb</machine>
      <machine maxCpus='288'>pc-q35-2.10</machine>
      <machine maxCpus='255'>pc-i440fx-1.7</machine>
      <machine maxCpus='255'>pc-0.14</machine>
      <machine maxCpus='288'>pc-q35-2.9</machine>
      <machine maxCpus='255'>pc-i440fx-2.11</machine>
      <machine maxCpus='288'>pc-q35-3.1</machine>
      <machine maxCpus='288'>pc-q35-4.1</machine>
      <machine maxCpus='255'>pc-i440fx-2.4</machine>
      <machine maxCpus='255'>pc-1.3</machine>
      <machine maxCpus='255'>pc-i440fx-4.1</machine>
      <machine maxCpus='288'>pc-q35-eoan</machine>
      <machine maxCpus='255'>pc-i440fx-2.9</machine>
      <machine maxCpus='255'>pc-i440fx-bionic-hpb</machine>
      <machine maxCpus='1'>isapc</machine>
      <machine maxCpus='255'>pc-i440fx-1.4</machine>
      <machine maxCpus='288'>pc-q35-cosmic</machine>
      <machine maxCpus='255'>pc-q35-2.6</machine>
      <machine maxCpus='255'>pc-i440fx-3.1</machine>
      <machine maxCpus='288'>pc-q35-bionic</machine>
      <machine maxCpus='288'>pc-q35-disco-hpb</machine>
      <machine maxCpus='255'>pc-i440fx-cosmic</machine>
      <machine maxCpus='288'>pc-q35-2.12</machine>
      <machine maxCpus='255'>pc-i440fx-bionic</machine>
      <machine maxCpus='288'>pc-q35-disco</machine>
      <machine maxCpus='255'>pc-i440fx-cosmic-hpb</machine>
      <machine maxCpus='255'>pc-i440fx-2.1</machine>
      <machine maxCpus='255'>pc-1.0</machine>
      <machine maxCpus='255'>pc-i440fx-wily</machine>
      <machine maxCpus='255'>pc-i440fx-2.6</machine>
      <machine maxCpus='288'>pc-q35-4.0.1</machine>
      <machine maxCpus='255'>pc-i440fx-1.6</machine>
      <machine maxCpus='255'>pc-0.13</machine>
      <machine maxCpus='288'>pc-q35-2.8</machine>
      <machine maxCpus='255'>pc-i440fx-2.10</machine>
      <machine maxCpus='288'>pc-q35-3.0</machine>
      <machine maxCpus='288'>pc-q35-zesty</machine>
      <machine maxCpus='288'>pc-q35-4.0</machine>
      <machine maxCpus='288'>microvm</machine>
      <machine maxCpus='255'>pc-i440fx-2.3</machine>
      <machine maxCpus='288'>pc-q35-focal</machine>
      <machine canonical='pc-q35-focal' maxCpus='288'>ubuntu-q35</machine>
      <machine maxCpus='255'>pc-i440fx-disco</machine>
      <machine maxCpus='255'>pc-1.2</machine>
      <machine maxCpus='255'>pc-i440fx-4.0</machine>
      <machine maxCpus='255'>pc-i440fx-focal-hpb</machine>
      <machine maxCpus='255'>pc-i440fx-2.8</machine>
      <machine maxCpus='255'>pc-i440fx-eoan</machine>
      <machine maxCpus='255'>pc-q35-2.5</machine>
      <machine maxCpus='255'>pc-i440fx-3.0</machine>
      <machine maxCpus='255'>pc-q35-yakkety</machine>
      <machine maxCpus='288'>pc-q35-2.11</machine>
      <domain type='qemu'/>
      <domain type='kvm'/>
    </arch>
    <features>
      <pae/>
      <nonpae/>
      <acpi default='on' toggle='yes'/>
      <apic default='on' toggle='no'/>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='lm32'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-lm32</emulator>
      <machine maxCpus='1'>lm32-evr</machine>
      <machine maxCpus='1'>lm32-uclinux</machine>
      <machine maxCpus='1'>milkymist</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='m68k'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-m68k</emulator>
      <machine maxCpus='1'>mcf5208evb</machine>
      <machine maxCpus='1'>an5206</machine>
      <machine maxCpus='1'>q800</machine>
      <machine maxCpus='1'>next-cube</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='microblaze'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-microblaze</emulator>
      <machine maxCpus='1'>petalogix-s3adsp1800</machine>
      <machine maxCpus='1'>petalogix-ml605</machine>
      <machine maxCpus='1'>xlnx-zynqmp-pmu</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='microblazeel'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-microblazeel</emulator>
      <machine maxCpus='1'>petalogix-s3adsp1800</machine>
      <machine maxCpus='1'>petalogix-ml605</machine>
      <machine maxCpus='1'>xlnx-zynqmp-pmu</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='mips'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-mips</emulator>
      <machine maxCpus='16'>malta</machine>
      <machine maxCpus='1'>mipssim</machine>
      <machine maxCpus='1'>mips</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='mipsel'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-mipsel</emulator>
      <machine maxCpus='16'>malta</machine>
      <machine maxCpus='1'>mipssim</machine>
      <machine maxCpus='1'>mips</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='mips64'>
      <wordsize>64</wordsize>
      <emulator>/usr/bin/qemu-system-mips64</emulator>
      <machine maxCpus='16'>malta</machine>
      <machine maxCpus='1'>pica61</machine>
      <machine maxCpus='1'>mipssim</machine>
      <machine maxCpus='1'>mips</machine>
      <machine maxCpus='1'>magnum</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='mips64el'>
      <wordsize>64</wordsize>
      <emulator>/usr/bin/qemu-system-mips64el</emulator>
      <machine maxCpus='16'>malta</machine>
      <machine maxCpus='1'>pica61</machine>
      <machine maxCpus='1'>mipssim</machine>
      <machine maxCpus='1'>mips</machine>
      <machine maxCpus='1'>fulong2e</machine>
      <machine maxCpus='16'>boston</machine>
      <machine maxCpus='1'>magnum</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='ppc'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-ppc</emulator>
      <machine maxCpus='1'>g3beige</machine>
      <machine maxCpus='1'>virtex-ml507</machine>
      <machine maxCpus='1'>mac99</machine>
      <machine maxCpus='32'>ppce500</machine>
      <machine maxCpus='1'>sam460ex</machine>
      <machine maxCpus='1'>bamboo</machine>
      <machine maxCpus='1'>prep</machine>
      <machine maxCpus='1'>40p</machine>
      <machine maxCpus='1'>ref405ep</machine>
      <machine maxCpus='15'>mpc8544ds</machine>
      <machine maxCpus='1'>taihu</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='ppc64'>
      <wordsize>64</wordsize>
      <emulator>/usr/bin/qemu-system-ppc64</emulator>
      <machine maxCpus='1024'>pseries-focal</machine>
      <machine canonical='pseries-focal' maxCpus='1024'>pseries</machine>
      <machine maxCpus='2048'>powernv9</machine>
      <machine canonical='powernv9' maxCpus='2048'>powernv</machine>
      <machine maxCpus='1'>taihu</machine>
      <machine maxCpus='1024'>pseries-4.1</machine>
      <machine maxCpus='15'>mpc8544ds</machine>
      <machine maxCpus='1024'>pseries-2.5</machine>
      <machine maxCpus='1024'>pseries-xenial</machine>
      <machine maxCpus='1024'>pseries-4.2</machine>
      <machine maxCpus='1024'>pseries-yakkety</machine>
      <machine maxCpus='1024'>pseries-2.6</machine>
      <machine maxCpus='32'>ppce500</machine>
      <machine maxCpus='1024'>pseries-bionic-sxxm</machine>
      <machine maxCpus='1024'>pseries-2.7</machine>
      <machine maxCpus='1024'>pseries-3.0</machine>
      <machine maxCpus='1'>40p</machine>
      <machine maxCpus='1024'>pseries-2.8</machine>
      <machine maxCpus='1024'>pseries-3.1</machine>
      <machine maxCpus='1024'>pseries-eoan</machine>
      <machine maxCpus='1024'>pseries-2.9</machine>
      <machine maxCpus='1024'>pseries-zesty</machine>
      <machine maxCpus='1'>bamboo</machine>
      <machine maxCpus='1'>g3beige</machine>
      <machine maxCpus='1024'>pseries-disco</machine>
      <machine maxCpus='1024'>pseries-2.12-sxxm</machine>
      <machine maxCpus='1024'>pseries-2.10</machine>
      <machine maxCpus='1'>virtex-ml507</machine>
      <machine maxCpus='1024'>pseries-2.11</machine>
      <machine maxCpus='1024'>pseries-2.1</machine>
      <machine maxCpus='1024'>pseries-cosmic</machine>
      <machine maxCpus='1024'>pseries-bionic</machine>
      <machine maxCpus='1024'>pseries-2.12</machine>
      <machine maxCpus='1024'>pseries-2.2</machine>
      <machine maxCpus='1'>mac99</machine>
      <machine maxCpus='1024'>pseries-artful</machine>
      <machine maxCpus='1'>sam460ex</machine>
      <machine maxCpus='1'>ref405ep</machine>
      <machine maxCpus='1024'>pseries-2.3</machine>
      <machine maxCpus='2048'>powernv8</machine>
      <machine maxCpus='1024'>pseries-4.0</machine>
      <machine maxCpus='1'>prep</machine>
      <machine maxCpus='1024'>pseries-2.4</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='ppc64le'>
      <wordsize>64</wordsize>
      <emulator>/usr/bin/qemu-system-ppc64le</emulator>
      <machine maxCpus='1024'>pseries-focal</machine>
      <machine canonical='pseries-focal' maxCpus='1024'>pseries</machine>
      <machine maxCpus='2048'>powernv9</machine>
      <machine canonical='powernv9' maxCpus='2048'>powernv</machine>
      <machine maxCpus='1'>taihu</machine>
      <machine maxCpus='1024'>pseries-4.1</machine>
      <machine maxCpus='15'>mpc8544ds</machine>
      <machine maxCpus='1024'>pseries-2.5</machine>
      <machine maxCpus='1024'>pseries-xenial</machine>
      <machine maxCpus='1024'>pseries-4.2</machine>
      <machine maxCpus='1024'>pseries-yakkety</machine>
      <machine maxCpus='1024'>pseries-2.6</machine>
      <machine maxCpus='32'>ppce500</machine>
      <machine maxCpus='1024'>pseries-bionic-sxxm</machine>
      <machine maxCpus='1024'>pseries-2.7</machine>
      <machine maxCpus='1024'>pseries-3.0</machine>
      <machine maxCpus='1'>40p</machine>
      <machine maxCpus='1024'>pseries-2.8</machine>
      <machine maxCpus='1024'>pseries-3.1</machine>
      <machine maxCpus='1024'>pseries-eoan</machine>
      <machine maxCpus='1024'>pseries-2.9</machine>
      <machine maxCpus='1024'>pseries-zesty</machine>
      <machine maxCpus='1'>bamboo</machine>
      <machine maxCpus='1'>g3beige</machine>
      <machine maxCpus='1024'>pseries-disco</machine>
      <machine maxCpus='1024'>pseries-2.12-sxxm</machine>
      <machine maxCpus='1024'>pseries-2.10</machine>
      <machine maxCpus='1'>virtex-ml507</machine>
      <machine maxCpus='1024'>pseries-2.11</machine>
      <machine maxCpus='1024'>pseries-2.1</machine>
      <machine maxCpus='1024'>pseries-cosmic</machine>
      <machine maxCpus='1024'>pseries-bionic</machine>
      <machine maxCpus='1024'>pseries-2.12</machine>
      <machine maxCpus='1024'>pseries-2.2</machine>
      <machine maxCpus='1'>mac99</machine>
      <machine maxCpus='1024'>pseries-artful</machine>
      <machine maxCpus='1'>sam460ex</machine>
      <machine maxCpus='1'>ref405ep</machine>
      <machine maxCpus='1024'>pseries-2.3</machine>
      <machine maxCpus='2048'>powernv8</machine>
      <machine maxCpus='1024'>pseries-4.0</machine>
      <machine maxCpus='1'>prep</machine>
      <machine maxCpus='1024'>pseries-2.4</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='riscv32'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-riscv32</emulator>
      <machine maxCpus='1'>spike_v1.10</machine>
      <machine maxCpus='8'>virt</machine>
      <machine maxCpus='1'>spike</machine>
      <machine maxCpus='1'>sifive_e</machine>
      <machine maxCpus='5'>sifive_u</machine>
      <machine maxCpus='1'>spike_v1.9.1</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='riscv64'>
      <wordsize>64</wordsize>
      <emulator>/usr/bin/qemu-system-riscv64</emulator>
      <machine maxCpus='1'>spike_v1.10</machine>
      <machine maxCpus='8'>virt</machine>
      <machine maxCpus='1'>spike</machine>
      <machine maxCpus='1'>sifive_e</machine>
      <machine maxCpus='5'>sifive_u</machine>
      <machine maxCpus='1'>spike_v1.9.1</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='s390x'>
      <wordsize>64</wordsize>
      <emulator>/usr/bin/qemu-system-s390x</emulator>
      <machine maxCpus='248'>s390-ccw-virtio-focal</machine>
      <machine canonical='s390-ccw-virtio-focal' maxCpus='248'>s390-ccw-virtio</machine>
      <machine maxCpus='248'>s390-ccw-virtio-4.0</machine>
      <machine maxCpus='248'>s390-ccw-virtio-artful</machine>
      <machine maxCpus='248'>s390-ccw-virtio-3.1</machine>
      <machine maxCpus='248'>s390-ccw-virtio-2.6</machine>
      <machine maxCpus='248'>s390-ccw-virtio-disco</machine>
      <machine maxCpus='248'>s390-ccw-virtio-2.12</machine>
      <machine maxCpus='248'>s390-ccw-virtio-yakkety</machine>
      <machine maxCpus='248'>s390-ccw-virtio-2.9</machine>
      <machine maxCpus='248'>s390-ccw-virtio-eoan</machine>
      <machine maxCpus='248'>s390-ccw-virtio-3.0</machine>
      <machine maxCpus='248'>s390-ccw-virtio-4.2</machine>
      <machine maxCpus='248'>s390-ccw-virtio-2.5</machine>
      <machine maxCpus='248'>s390-ccw-virtio-2.11</machine>
      <machine maxCpus='248'>s390-ccw-virtio-xenial</machine>
      <machine maxCpus='248'>s390-ccw-virtio-2.8</machine>
      <machine maxCpus='248'>s390-ccw-virtio-bionic</machine>
      <machine maxCpus='248'>s390-ccw-virtio-zesty</machine>
      <machine maxCpus='248'>s390-ccw-virtio-4.1</machine>
      <machine maxCpus='248'>s390-ccw-virtio-cosmic</machine>
      <machine maxCpus='248'>s390-ccw-virtio-2.4</machine>
      <machine maxCpus='248'>s390-ccw-virtio-2.10</machine>
      <machine maxCpus='248'>s390-ccw-virtio-2.7</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='sh4'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-sh4</emulator>
      <machine maxCpus='1'>shix</machine>
      <machine maxCpus='1'>r2d</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='sh4eb'>
      <wordsize>64</wordsize>
      <emulator>/usr/bin/qemu-system-sh4eb</emulator>
      <machine maxCpus='1'>shix</machine>
      <machine maxCpus='1'>r2d</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='sparc'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-sparc</emulator>
      <machine maxCpus='1'>SS-5</machine>
      <machine maxCpus='4'>SS-20</machine>
      <machine maxCpus='1'>LX</machine>
      <machine maxCpus='1'>SPARCClassic</machine>
      <machine maxCpus='1'>leon3_generic</machine>
      <machine maxCpus='1'>SPARCbook</machine>
      <machine maxCpus='1'>SS-4</machine>
      <machine maxCpus='4'>SS-600MP</machine>
      <machine maxCpus='4'>SS-10</machine>
      <machine maxCpus='1'>Voyager</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='sparc64'>
      <wordsize>64</wordsize>
      <emulator>/usr/bin/qemu-system-sparc64</emulator>
      <machine maxCpus='1'>sun4u</machine>
      <machine maxCpus='1'>niagara</machine>
      <machine maxCpus='1'>sun4v</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='unicore32'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-unicore32</emulator>
      <machine maxCpus='1'>puv3</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='x86_64'>
      <wordsize>64</wordsize>
      <emulator>/usr/bin/qemu-system-x86_64</emulator>
      <machine maxCpus='255'>pc-i440fx-focal</machine>
      <machine canonical='pc-i440fx-focal' maxCpus='255'>ubuntu</machine>
      <machine maxCpus='255'>pc-0.15</machine>
      <machine maxCpus='255'>pc-i440fx-2.12</machine>
      <machine maxCpus='255'>pc-i440fx-2.0</machine>
      <machine maxCpus='255'>pc-i440fx-xenial</machine>
      <machine maxCpus='288'>pc-q35-4.2</machine>
      <machine canonical='pc-q35-4.2' maxCpus='288'>q35</machine>
      <machine maxCpus='255'>pc-i440fx-2.5</machine>
      <machine maxCpus='255'>pc-i440fx-4.2</machine>
      <machine canonical='pc-i440fx-4.2' maxCpus='255'>pc</machine>
      <machine maxCpus='255'>pc-q35-xenial</machine>
      <machine maxCpus='255'>pc-i440fx-1.5</machine>
      <machine maxCpus='255'>pc-q35-2.7</machine>
      <machine maxCpus='255'>pc-0.12</machine>
      <machine maxCpus='288'>pc-q35-eoan-hpb</machine>
      <machine maxCpus='255'>pc-i440fx-zesty</machine>
      <machine maxCpus='255'>pc-i440fx-disco-hpb</machine>
      <machine maxCpus='288'>pc-q35-artful</machine>
      <machine maxCpus='255'>pc-i440fx-trusty</machine>
      <machine maxCpus='255'>pc-i440fx-2.2</machine>
      <machine maxCpus='255'>pc-i440fx-eoan-hpb</machine>
      <machine maxCpus='288'>pc-q35-focal-hpb</machine>
      <machine maxCpus='255'>pc-1.1</machine>
      <machine maxCpus='288'>pc-q35-bionic-hpb</machine>
      <machine maxCpus='255'>pc-i440fx-artful</machine>
      <machine maxCpus='255'>pc-i440fx-2.7</machine>
      <machine maxCpus='255'>pc-i440fx-yakkety</machine>
      <machine maxCpus='255'>pc-q35-2.4</machine>
      <machine maxCpus='288'>pc-q35-cosmic-hpb</machine>
      <machine maxCpus='288'>pc-q35-2.10</machine>
      <machine maxCpus='255'>pc-i440fx-1.7</machine>
      <machine maxCpus='255'>pc-0.14</machine>
      <machine maxCpus='288'>pc-q35-2.9</machine>
      <machine maxCpus='255'>pc-i440fx-2.11</machine>
      <machine maxCpus='288'>pc-q35-3.1</machine>
      <machine maxCpus='288'>pc-q35-4.1</machine>
      <machine maxCpus='255'>pc-i440fx-2.4</machine>
      <machine maxCpus='255'>pc-1.3</machine>
      <machine maxCpus='255'>pc-i440fx-4.1</machine>
      <machine maxCpus='288'>pc-q35-eoan</machine>
      <machine maxCpus='255'>pc-i440fx-2.9</machine>
      <machine maxCpus='255'>pc-i440fx-bionic-hpb</machine>
      <machine maxCpus='1'>isapc</machine>
      <machine maxCpus='255'>pc-i440fx-1.4</machine>
      <machine maxCpus='288'>pc-q35-cosmic</machine>
      <machine maxCpus='255'>pc-q35-2.6</machine>
      <machine maxCpus='288'>pc-q35-bionic</machine>
      <machine maxCpus='255'>pc-i440fx-3.1</machine>
      <machine maxCpus='288'>pc-q35-disco-hpb</machine>
      <machine maxCpus='255'>pc-i440fx-cosmic</machine>
      <machine maxCpus='288'>pc-q35-2.12</machine>
      <machine maxCpus='255'>pc-i440fx-bionic</machine>
      <machine maxCpus='288'>pc-q35-disco</machine>
      <machine maxCpus='255'>pc-i440fx-cosmic-hpb</machine>
      <machine maxCpus='255'>pc-i440fx-2.1</machine>
      <machine maxCpus='255'>pc-1.0</machine>
      <machine maxCpus='255'>pc-i440fx-wily</machine>
      <machine maxCpus='255'>pc-i440fx-2.6</machine>
      <machine maxCpus='288'>pc-q35-4.0.1</machine>
      <machine maxCpus='255'>pc-i440fx-1.6</machine>
      <machine maxCpus='255'>pc-0.13</machine>
      <machine maxCpus='288'>pc-q35-2.8</machine>
      <machine maxCpus='255'>pc-i440fx-2.10</machine>
      <machine maxCpus='288'>pc-q35-3.0</machine>
      <machine maxCpus='288'>pc-q35-zesty</machine>
      <machine maxCpus='288'>pc-q35-4.0</machine>
      <machine maxCpus='288'>microvm</machine>
      <machine maxCpus='288'>pc-q35-focal</machine>
      <machine canonical='pc-q35-focal' maxCpus='288'>ubuntu-q35</machine>
      <machine maxCpus='255'>pc-i440fx-2.3</machine>
      <machine maxCpus='255'>pc-i440fx-focal-hpb</machine>
      <machine maxCpus='255'>pc-1.2</machine>
      <machine maxCpus='255'>pc-i440fx-4.0</machine>
      <machine maxCpus='255'>pc-i440fx-disco</machine>
      <machine maxCpus='255'>pc-i440fx-2.8</machine>
      <machine maxCpus='255'>pc-i440fx-eoan</machine>
      <machine maxCpus='255'>pc-q35-2.5</machine>
      <machine maxCpus='255'>pc-i440fx-3.0</machine>
      <machine maxCpus='255'>pc-q35-yakkety</machine>
      <machine maxCpus='288'>pc-q35-2.11</machine>
      <domain type='qemu'/>
      <domain type='kvm'/>
    </arch>
    <features>
      <acpi default='on' toggle='yes'/>
      <apic default='on' toggle='no'/>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='xtensa'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-xtensa</emulator>
      <machine maxCpus='4'>sim</machine>
      <machine maxCpus='32'>kc705</machine>
      <machine maxCpus='32'>ml605</machine>
      <machine maxCpus='32'>ml605-nommu</machine>
      <machine maxCpus='32'>virt</machine>
      <machine maxCpus='32'>lx60-nommu</machine>
      <machine maxCpus='32'>lx200</machine>
      <machine maxCpus='32'>lx200-nommu</machine>
      <machine maxCpus='32'>lx60</machine>
      <machine maxCpus='32'>kc705-nommu</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='xtensaeb'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-xtensaeb</emulator>
      <machine maxCpus='4'>sim</machine>
      <machine maxCpus='32'>kc705</machine>
      <machine maxCpus='32'>ml605</machine>
      <machine maxCpus='32'>ml605-nommu</machine>
      <machine maxCpus='32'>virt</machine>
      <machine maxCpus='32'>lx60-nommu</machine>
      <machine maxCpus='32'>lx200</machine>
      <machine maxCpus='32'>lx200-nommu</machine>
      <machine maxCpus='32'>lx60</machine>
      <machine maxCpus='32'>kc705-nommu</machine>
      <domain type='qemu'/>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <disksnapshot default='on' toggle='no'/>
    </features>
  </guest>

</capabilities>

2022-02-21 19:19:34.632 7 INFO nova.virt.libvirt.host [-] Secure Boot support detected
2022-02-21 19:19:34.646 7 WARNING nova.virt.libvirt.driver [-] Running Nova with a libvirt version less than 7.0.0 is deprecated. The required minimum version of libvirt will be raised to 7.0.0 in the next release.
2022-02-21 19:19:34.648 7 WARNING nova.virt.libvirt.driver [-] Running Nova with a QEMU version less than 5.2.0 is deprecated. The required minimum version of QEMU will be raised to 5.2.0 in the next release.
2022-02-21 19:19:35.044 7 INFO nova.compute.manager [req-fb59e066-5deb-4d78-a9a9-cc87d81c94af - - - - -] Looking for unclaimed instances stuck in BUILDING status for nodes managed by this host
2022-02-21 19:19:35.174 7 INFO nova.service [req-fb59e066-5deb-4d78-a9a9-cc87d81c94af - - - - -] Updating service version for nova-compute on rack08-server63 from 61 to 60
2022-02-21 19:19:36.992 7 INFO nova.virt.libvirt.host [req-fb59e066-5deb-4d78-a9a9-cc87d81c94af - - - - -] kernel doesn't support AMD SEV
2022-02-21 19:20:31.054 7 INFO nova.compute.claims [req-67ae9861-526a-49f4-bb6f-4c0451209118 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 206edc67-4e73-4b9a-9188-42e86bef0c37] Claim successful on node rack08-server63
2022-02-21 19:20:31.470 7 INFO nova.virt.libvirt.driver [req-67ae9861-526a-49f4-bb6f-4c0451209118 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 206edc67-4e73-4b9a-9188-42e86bef0c37] Creating image
2022-02-21 19:20:31.474 7 INFO oslo.privsep.daemon [req-67ae9861-526a-49f4-bb6f-4c0451209118 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] Running privsep helper: ['sudo', 'nova-rootwrap', '/etc/nova/rootwrap.conf', 'privsep-helper', '--config-file', '/etc/nova/nova.conf', '--privsep_context', 'nova.privsep.sys_admin_pctxt', '--privsep_sock_path', '/tmp/tmpk4hzye11/privsep.sock']
2022-02-21 19:20:33.106 7 INFO oslo.privsep.daemon [req-67ae9861-526a-49f4-bb6f-4c0451209118 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] Spawned new privsep daemon via rootwrap
2022-02-21 19:20:32.961 80 INFO oslo.privsep.daemon [-] privsep daemon starting
2022-02-21 19:20:32.967 80 INFO oslo.privsep.daemon [-] privsep process running with uid/gid: 0/0
2022-02-21 19:20:32.972 80 INFO oslo.privsep.daemon [-] privsep process running with capabilities (eff/prm/inh): CAP_CHOWN|CAP_DAC_OVERRIDE|CAP_DAC_READ_SEARCH|CAP_FOWNER|CAP_NET_ADMIN|CAP_SYS_ADMIN/CAP_CHOWN|CAP_DAC_OVERRIDE|CAP_DAC_READ_SEARCH|CAP_FOWNER|CAP_NET_ADMIN|CAP_SYS_ADMIN/none
2022-02-21 19:20:32.972 80 INFO oslo.privsep.daemon [-] privsep daemon running as pid 80
2022-02-21 19:20:34.378 7 INFO nova.compute.manager [-] [instance: 206edc67-4e73-4b9a-9188-42e86bef0c37] VM Resumed (Lifecycle Event)
2022-02-21 19:20:34.384 7 INFO nova.virt.libvirt.driver [-] [instance: 206edc67-4e73-4b9a-9188-42e86bef0c37] Instance spawned successfully.
2022-02-21 19:20:34.385 7 INFO nova.compute.manager [req-67ae9861-526a-49f4-bb6f-4c0451209118 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 206edc67-4e73-4b9a-9188-42e86bef0c37] Took 2.92 seconds to spawn the instance on the hypervisor.
2022-02-21 19:20:34.433 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 206edc67-4e73-4b9a-9188-42e86bef0c37] During sync_power_state the instance has a pending task (spawning). Skip.
2022-02-21 19:20:34.434 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 206edc67-4e73-4b9a-9188-42e86bef0c37] VM Started (Lifecycle Event)
2022-02-21 19:20:34.478 7 INFO nova.compute.manager [req-67ae9861-526a-49f4-bb6f-4c0451209118 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 206edc67-4e73-4b9a-9188-42e86bef0c37] Took 3.47 seconds to build instance.
2022-02-21 19:20:37.686 7 INFO nova.compute.manager [req-4e60b193-b5b6-41b8-8097-e1a85a3bc6cb 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 206edc67-4e73-4b9a-9188-42e86bef0c37] Attaching volume d2f66b4d-62c9-4f1f-9f94-ce62f73bf0dc to /dev/vdb
2022-02-21 19:20:37.764 7 INFO oslo.privsep.daemon [req-4e60b193-b5b6-41b8-8097-e1a85a3bc6cb 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] Running privsep helper: ['sudo', '-E', 'nova-rootwrap', '/etc/nova/rootwrap.conf', 'privsep-helper', '--config-file', '/etc/nova/nova.conf', '--privsep_context', 'os_brick.privileged.default', '--privsep_sock_path', '/tmp/tmphpy_2fzm/privsep.sock']
2022-02-21 19:20:38.434 7 INFO oslo.privsep.daemon [req-4e60b193-b5b6-41b8-8097-e1a85a3bc6cb 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] Spawned new privsep daemon via rootwrap
2022-02-21 19:20:38.351 120 INFO oslo.privsep.daemon [-] privsep daemon starting
2022-02-21 19:20:38.358 120 INFO oslo.privsep.daemon [-] privsep process running with uid/gid: 0/0
2022-02-21 19:20:38.363 120 INFO oslo.privsep.daemon [-] privsep process running with capabilities (eff/prm/inh): CAP_DAC_READ_SEARCH|CAP_SYS_ADMIN/CAP_DAC_READ_SEARCH|CAP_SYS_ADMIN/none
2022-02-21 19:20:38.364 120 INFO oslo.privsep.daemon [-] privsep daemon running as pid 120
2022-02-21 19:20:38.759 7 WARNING os_brick.initiator.connectors.nvmeof [req-4e60b193-b5b6-41b8-8097-e1a85a3bc6cb 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] Process execution error in _get_host_uuid: Unexpected error while running command.
Command: blkid overlay -s UUID -o value
Exit code: 2
Stdout: ''
Stderr: '': oslo_concurrency.processutils.ProcessExecutionError: Unexpected error while running command.
2022-02-21 19:20:40.148 7 INFO os_brick.initiator.connectors.lightos [req-4e60b193-b5b6-41b8-8097-e1a85a3bc6cb 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] LIGHTOS: connect_volume called for volume ddb4f28c-2ec9-49b0-b895-b4193ccdcf9d, connection properties: {'lightos_nodes': {'[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}}, 'uuid': 'ddb4f28c-2ec9-49b0-b895-b4193ccdcf9d', 'subsysnqn': 'nqn.2016-01.com.lightbitslabs:uuid:2df610f7-a728-4fa9-8ec0-3ecb55dad510', 'qos_specs': None, 'access_mode': 'rw', 'encrypted': False, 'cacheable': False}
2022-02-21 19:20:40.183 7 INFO os_brick.initiator.connectors.lightos [req-4e60b193-b5b6-41b8-8097-e1a85a3bc6cb 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid ddb4f28c-2ec9-49b0-b895-b4193ccdcf9d
2022-02-21 19:20:42.357 7 INFO nova.compute.claims [req-0adae8e6-1732-4ec6-95c8-05b0dd5f40f8 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 104de1d5-97a2-47e6-ac83-48561650dd3f] Claim successful on node rack08-server63
2022-02-21 19:20:42.756 7 INFO nova.virt.libvirt.driver [req-0adae8e6-1732-4ec6-95c8-05b0dd5f40f8 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 104de1d5-97a2-47e6-ac83-48561650dd3f] Creating image
2022-02-21 19:20:43.895 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 104de1d5-97a2-47e6-ac83-48561650dd3f] VM Resumed (Lifecycle Event)
2022-02-21 19:20:43.902 7 INFO nova.virt.libvirt.driver [-] [instance: 104de1d5-97a2-47e6-ac83-48561650dd3f] Instance spawned successfully.
2022-02-21 19:20:43.903 7 INFO nova.compute.manager [req-0adae8e6-1732-4ec6-95c8-05b0dd5f40f8 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 104de1d5-97a2-47e6-ac83-48561650dd3f] Took 1.15 seconds to spawn the instance on the hypervisor.
2022-02-21 19:20:43.953 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 104de1d5-97a2-47e6-ac83-48561650dd3f] During sync_power_state the instance has a pending task (spawning). Skip.
2022-02-21 19:20:43.954 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 104de1d5-97a2-47e6-ac83-48561650dd3f] VM Started (Lifecycle Event)
2022-02-21 19:20:43.992 7 INFO nova.compute.manager [req-0adae8e6-1732-4ec6-95c8-05b0dd5f40f8 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 104de1d5-97a2-47e6-ac83-48561650dd3f] Took 1.68 seconds to build instance.
2022-02-21 19:20:45.225 7 INFO nova.compute.claims [req-4f6c8355-cbcc-47f9-ad58-2585e13b2b7a 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 6b416943-2b85-4dfa-b133-e9d5ec5d39fd] Claim successful on node rack08-server63
2022-02-21 19:20:45.467 7 INFO nova.virt.libvirt.driver [req-4f6c8355-cbcc-47f9-ad58-2585e13b2b7a 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 6b416943-2b85-4dfa-b133-e9d5ec5d39fd] Ignoring supplied device name: /dev/vda. Libvirt can't honour user-supplied dev names
2022-02-21 19:20:45.499 7 INFO nova.compute.manager [req-75e95593-319f-44dd-9a0a-ec134ace0783 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 206edc67-4e73-4b9a-9188-42e86bef0c37] Detaching volume d2f66b4d-62c9-4f1f-9f94-ce62f73bf0dc
2022-02-21 19:20:45.556 7 INFO nova.virt.block_device [req-4f6c8355-cbcc-47f9-ad58-2585e13b2b7a 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 6b416943-2b85-4dfa-b133-e9d5ec5d39fd] Booting with volume eaaabbfa-c1f8-4dac-9e41-b97562de7ddd at /dev/vda
2022-02-21 19:20:45.564 7 INFO nova.virt.block_device [req-75e95593-319f-44dd-9a0a-ec134ace0783 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 206edc67-4e73-4b9a-9188-42e86bef0c37] Attempting to driver detach volume d2f66b4d-62c9-4f1f-9f94-ce62f73bf0dc from mountpoint /dev/vdb
2022-02-21 19:20:45.584 7 INFO nova.virt.libvirt.driver [req-75e95593-319f-44dd-9a0a-ec134ace0783 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] Successfully detached device vdb from instance 206edc67-4e73-4b9a-9188-42e86bef0c37 from the persistent domain config.
2022-02-21 19:20:45.639 7 WARNING os_brick.initiator.connectors.nvmeof [req-4f6c8355-cbcc-47f9-ad58-2585e13b2b7a 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] Process execution error in _get_host_uuid: Unexpected error while running command.
Command: blkid overlay -s UUID -o value
Exit code: 2
Stdout: ''
Stderr: '': oslo_concurrency.processutils.ProcessExecutionError: Unexpected error while running command.
2022-02-21 19:20:45.723 7 INFO nova.virt.libvirt.driver [req-75e95593-319f-44dd-9a0a-ec134ace0783 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] Successfully detached device vdb from instance 206edc67-4e73-4b9a-9188-42e86bef0c37 from the live domain config.
2022-02-21 19:20:45.726 7 INFO os_brick.initiator.connectors.lightos [req-75e95593-319f-44dd-9a0a-ec134ace0783 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] LIGHTOS: connect_volume called for volume ddb4f28c-2ec9-49b0-b895-b4193ccdcf9d, connection properties: {'lightos_nodes': {'[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}}, 'uuid': 'ddb4f28c-2ec9-49b0-b895-b4193ccdcf9d', 'subsysnqn': 'nqn.2016-01.com.lightbitslabs:uuid:2df610f7-a728-4fa9-8ec0-3ecb55dad510', 'qos_specs': None, 'access_mode': 'rw', 'encrypted': False, 'cacheable': False, 'device_path': '/dev/nvme0n1'}
2022-02-21 19:20:45.727 7 INFO os_brick.initiator.connectors.lightos [req-75e95593-319f-44dd-9a0a-ec134ace0783 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid ddb4f28c-2ec9-49b0-b895-b4193ccdcf9d
2022-02-21 19:20:45.728 7 INFO os_brick.initiator.connectors.lightos [req-75e95593-319f-44dd-9a0a-ec134ace0783 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid ddb4f28c-2ec9-49b0-b895-b4193ccdcf9d
2022-02-21 19:20:47.150 7 INFO nova.virt.libvirt.driver [req-4f6c8355-cbcc-47f9-ad58-2585e13b2b7a 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 6b416943-2b85-4dfa-b133-e9d5ec5d39fd] Creating image
2022-02-21 19:20:47.161 7 INFO os_brick.initiator.connectors.lightos [req-4f6c8355-cbcc-47f9-ad58-2585e13b2b7a 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] LIGHTOS: connect_volume called for volume c7aed01c-2118-4e01-9fa5-d25f08ec854e, connection properties: {'lightos_nodes': {'[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}}, 'uuid': 'c7aed01c-2118-4e01-9fa5-d25f08ec854e', 'subsysnqn': 'nqn.2016-01.com.lightbitslabs:uuid:2df610f7-a728-4fa9-8ec0-3ecb55dad510', 'qos_specs': None, 'access_mode': 'rw', 'encrypted': False, 'cacheable': False}
2022-02-21 19:20:47.164 7 INFO os_brick.initiator.connectors.lightos [req-4f6c8355-cbcc-47f9-ad58-2585e13b2b7a 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] LIGHTOS: devpath /dev/nvme0n3 detected for uuid c7aed01c-2118-4e01-9fa5-d25f08ec854e
2022-02-21 19:20:47.986 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 6b416943-2b85-4dfa-b133-e9d5ec5d39fd] VM Resumed (Lifecycle Event)
2022-02-21 19:20:47.994 7 INFO nova.virt.libvirt.driver [-] [instance: 6b416943-2b85-4dfa-b133-e9d5ec5d39fd] Instance spawned successfully.
2022-02-21 19:20:47.994 7 INFO nova.compute.manager [req-4f6c8355-cbcc-47f9-ad58-2585e13b2b7a 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 6b416943-2b85-4dfa-b133-e9d5ec5d39fd] Took 0.85 seconds to spawn the instance on the hypervisor.
2022-02-21 19:20:48.040 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 6b416943-2b85-4dfa-b133-e9d5ec5d39fd] During sync_power_state the instance has a pending task (spawning). Skip.
2022-02-21 19:20:48.041 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 6b416943-2b85-4dfa-b133-e9d5ec5d39fd] VM Started (Lifecycle Event)
2022-02-21 19:20:48.081 7 INFO nova.compute.manager [req-4f6c8355-cbcc-47f9-ad58-2585e13b2b7a 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 6b416943-2b85-4dfa-b133-e9d5ec5d39fd] Took 2.89 seconds to build instance.
2022-02-21 19:20:49.047 7 INFO nova.compute.claims [req-804899b3-215b-42ae-b2da-fc6bdfd6b5a7 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 89b44f49-5340-47ce-b489-6ceafcad65e2] Claim successful on node rack08-server63
2022-02-21 19:20:49.439 7 INFO nova.virt.libvirt.driver [req-804899b3-215b-42ae-b2da-fc6bdfd6b5a7 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 89b44f49-5340-47ce-b489-6ceafcad65e2] Creating image
2022-02-21 19:20:50.403 7 INFO nova.compute.claims [req-efd4b6e2-5bd1-4ef4-af11-8dd1aeeca984 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 588650e2-0ecb-4684-bd71-53f95481f013] Claim successful on node rack08-server63
2022-02-21 19:20:50.632 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 89b44f49-5340-47ce-b489-6ceafcad65e2] VM Resumed (Lifecycle Event)
2022-02-21 19:20:50.639 7 INFO nova.virt.libvirt.driver [-] [instance: 89b44f49-5340-47ce-b489-6ceafcad65e2] Instance spawned successfully.
2022-02-21 19:20:50.639 7 INFO nova.compute.manager [req-804899b3-215b-42ae-b2da-fc6bdfd6b5a7 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 89b44f49-5340-47ce-b489-6ceafcad65e2] Took 1.20 seconds to spawn the instance on the hypervisor.
2022-02-21 19:20:50.687 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 89b44f49-5340-47ce-b489-6ceafcad65e2] During sync_power_state the instance has a pending task (spawning). Skip.
2022-02-21 19:20:50.688 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 89b44f49-5340-47ce-b489-6ceafcad65e2] VM Started (Lifecycle Event)
2022-02-21 19:20:50.728 7 INFO nova.compute.manager [req-804899b3-215b-42ae-b2da-fc6bdfd6b5a7 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 89b44f49-5340-47ce-b489-6ceafcad65e2] Took 1.72 seconds to build instance.
2022-02-21 19:20:50.783 7 INFO nova.virt.libvirt.driver [req-efd4b6e2-5bd1-4ef4-af11-8dd1aeeca984 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 588650e2-0ecb-4684-bd71-53f95481f013] Creating image
2022-02-21 19:20:50.976 7 INFO nova.compute.claims [req-5c958030-c275-4a42-861b-2cfd13f3361c 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Claim successful on node rack08-server63
2022-02-21 19:20:51.366 7 INFO nova.virt.libvirt.driver [req-5c958030-c275-4a42-861b-2cfd13f3361c 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Creating image
2022-02-21 19:20:51.928 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 588650e2-0ecb-4684-bd71-53f95481f013] VM Resumed (Lifecycle Event)
2022-02-21 19:20:51.936 7 INFO nova.virt.libvirt.driver [-] [instance: 588650e2-0ecb-4684-bd71-53f95481f013] Instance spawned successfully.
2022-02-21 19:20:51.937 7 INFO nova.compute.manager [req-efd4b6e2-5bd1-4ef4-af11-8dd1aeeca984 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 588650e2-0ecb-4684-bd71-53f95481f013] Took 1.16 seconds to spawn the instance on the hypervisor.
2022-02-21 19:20:51.983 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 588650e2-0ecb-4684-bd71-53f95481f013] During sync_power_state the instance has a pending task (spawning). Skip.
2022-02-21 19:20:51.983 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 588650e2-0ecb-4684-bd71-53f95481f013] VM Started (Lifecycle Event)
2022-02-21 19:20:52.021 7 INFO nova.compute.manager [req-efd4b6e2-5bd1-4ef4-af11-8dd1aeeca984 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 588650e2-0ecb-4684-bd71-53f95481f013] Took 1.66 seconds to build instance.
2022-02-21 19:20:52.577 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] VM Resumed (Lifecycle Event)
2022-02-21 19:20:52.584 7 INFO nova.virt.libvirt.driver [-] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Instance spawned successfully.
2022-02-21 19:20:52.584 7 INFO nova.compute.manager [req-5c958030-c275-4a42-861b-2cfd13f3361c 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Took 1.22 seconds to spawn the instance on the hypervisor.
2022-02-21 19:20:52.633 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] During sync_power_state the instance has a pending task (spawning). Skip.
2022-02-21 19:20:52.634 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] VM Started (Lifecycle Event)
2022-02-21 19:20:52.682 7 INFO nova.compute.manager [req-5c958030-c275-4a42-861b-2cfd13f3361c 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Took 1.76 seconds to build instance.
2022-02-21 19:20:53.480 7 INFO nova.compute.manager [req-5203e826-64ab-49bc-9fbd-522282f28cc4 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 89b44f49-5340-47ce-b489-6ceafcad65e2] Attaching volume 9b0238b3-85c7-4fed-8a06-fab37f971c56 to /dev/vdb
2022-02-21 19:20:53.566 7 WARNING os_brick.initiator.connectors.nvmeof [req-5203e826-64ab-49bc-9fbd-522282f28cc4 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] Process execution error in _get_host_uuid: Unexpected error while running command.
Command: blkid overlay -s UUID -o value
Exit code: 2
Stdout: ''
Stderr: '': oslo_concurrency.processutils.ProcessExecutionError: Unexpected error while running command.
2022-02-21 19:20:54.924 7 INFO os_brick.initiator.connectors.lightos [req-5203e826-64ab-49bc-9fbd-522282f28cc4 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] LIGHTOS: connect_volume called for volume 819b8381-c6a6-4bd5-bce1-8d902ca01ed5, connection properties: {'lightos_nodes': {'[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}}, 'uuid': '819b8381-c6a6-4bd5-bce1-8d902ca01ed5', 'subsysnqn': 'nqn.2016-01.com.lightbitslabs:uuid:2df610f7-a728-4fa9-8ec0-3ecb55dad510', 'qos_specs': None, 'access_mode': 'rw', 'encrypted': False, 'cacheable': False}
2022-02-21 19:20:54.929 7 INFO os_brick.initiator.connectors.lightos [req-5203e826-64ab-49bc-9fbd-522282f28cc4 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid 819b8381-c6a6-4bd5-bce1-8d902ca01ed5
2022-02-21 19:20:55.719 7 INFO nova.compute.manager [req-ba04e596-5547-4dba-998b-6694293b9e2f 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 588650e2-0ecb-4684-bd71-53f95481f013] Attaching volume eed44304-c133-4140-a247-184f48b1a812 to /dev/vdb
2022-02-21 19:20:55.804 7 WARNING os_brick.initiator.connectors.nvmeof [req-ba04e596-5547-4dba-998b-6694293b9e2f 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] Process execution error in _get_host_uuid: Unexpected error while running command.
Command: blkid overlay -s UUID -o value
Exit code: 2
Stdout: ''
Stderr: '': oslo_concurrency.processutils.ProcessExecutionError: Unexpected error while running command.
2022-02-21 19:20:56.106 7 INFO nova.compute.manager [req-6bccb48b-8aa2-4fee-963f-117300ead75e 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 89b44f49-5340-47ce-b489-6ceafcad65e2] Detaching volume 9b0238b3-85c7-4fed-8a06-fab37f971c56
2022-02-21 19:20:56.166 7 INFO nova.virt.block_device [req-6bccb48b-8aa2-4fee-963f-117300ead75e 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 89b44f49-5340-47ce-b489-6ceafcad65e2] Attempting to driver detach volume 9b0238b3-85c7-4fed-8a06-fab37f971c56 from mountpoint /dev/vdb
2022-02-21 19:20:56.187 7 INFO nova.virt.libvirt.driver [req-6bccb48b-8aa2-4fee-963f-117300ead75e 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] Successfully detached device vdb from instance 89b44f49-5340-47ce-b489-6ceafcad65e2 from the persistent domain config.
2022-02-21 19:20:56.327 7 INFO nova.virt.libvirt.driver [req-6bccb48b-8aa2-4fee-963f-117300ead75e 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] Successfully detached device vdb from instance 89b44f49-5340-47ce-b489-6ceafcad65e2 from the live domain config.
2022-02-21 19:20:56.330 7 INFO os_brick.initiator.connectors.lightos [req-6bccb48b-8aa2-4fee-963f-117300ead75e 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] LIGHTOS: connect_volume called for volume 819b8381-c6a6-4bd5-bce1-8d902ca01ed5, connection properties: {'lightos_nodes': {'[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}}, 'uuid': '819b8381-c6a6-4bd5-bce1-8d902ca01ed5', 'subsysnqn': 'nqn.2016-01.com.lightbitslabs:uuid:2df610f7-a728-4fa9-8ec0-3ecb55dad510', 'qos_specs': None, 'access_mode': 'rw', 'encrypted': False, 'cacheable': False, 'device_path': '/dev/nvme0n1'}
2022-02-21 19:20:56.331 7 INFO os_brick.initiator.connectors.lightos [req-6bccb48b-8aa2-4fee-963f-117300ead75e 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid 819b8381-c6a6-4bd5-bce1-8d902ca01ed5
2022-02-21 19:20:56.332 7 INFO os_brick.initiator.connectors.lightos [req-6bccb48b-8aa2-4fee-963f-117300ead75e 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid 819b8381-c6a6-4bd5-bce1-8d902ca01ed5
2022-02-21 19:20:57.138 7 INFO os_brick.initiator.connectors.lightos [req-ba04e596-5547-4dba-998b-6694293b9e2f 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] LIGHTOS: connect_volume called for volume a2eb3747-ae4b-4f62-ab66-6392b60865ea, connection properties: {'lightos_nodes': {'[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}}, 'uuid': 'a2eb3747-ae4b-4f62-ab66-6392b60865ea', 'subsysnqn': 'nqn.2016-01.com.lightbitslabs:uuid:2df610f7-a728-4fa9-8ec0-3ecb55dad510', 'qos_specs': None, 'access_mode': 'rw', 'encrypted': False, 'cacheable': False}
2022-02-21 19:20:57.142 7 INFO os_brick.initiator.connectors.lightos [req-ba04e596-5547-4dba-998b-6694293b9e2f 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] LIGHTOS: devpath /dev/nvme0n4 detected for uuid a2eb3747-ae4b-4f62-ab66-6392b60865ea
2022-02-21 19:20:58.265 7 INFO nova.compute.manager [req-19f0fb3e-0cee-4d45-a79a-d4969e45380a 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Attaching volume eed44304-c133-4140-a247-184f48b1a812 to /dev/vdb
2022-02-21 19:20:58.354 7 WARNING os_brick.initiator.connectors.nvmeof [req-19f0fb3e-0cee-4d45-a79a-d4969e45380a 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] Process execution error in _get_host_uuid: Unexpected error while running command.
Command: blkid overlay -s UUID -o value
Exit code: 2
Stdout: ''
Stderr: '': oslo_concurrency.processutils.ProcessExecutionError: Unexpected error while running command.
2022-02-21 19:20:59.455 7 INFO nova.compute.claims [req-7923e58c-001e-4ded-a92e-4be904f15752 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 7480ab10-22ba-4886-a571-f77c60fda1bf] Claim successful on node rack08-server63
2022-02-21 19:20:59.723 7 INFO os_brick.initiator.connectors.lightos [req-19f0fb3e-0cee-4d45-a79a-d4969e45380a 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] LIGHTOS: connect_volume called for volume a2eb3747-ae4b-4f62-ab66-6392b60865ea, connection properties: {'lightos_nodes': {'[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}}, 'uuid': 'a2eb3747-ae4b-4f62-ab66-6392b60865ea', 'subsysnqn': 'nqn.2016-01.com.lightbitslabs:uuid:2df610f7-a728-4fa9-8ec0-3ecb55dad510', 'qos_specs': None, 'access_mode': 'rw', 'encrypted': False, 'cacheable': False}
2022-02-21 19:20:59.724 7 INFO os_brick.initiator.connectors.lightos [req-19f0fb3e-0cee-4d45-a79a-d4969e45380a 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] LIGHTOS: devpath /dev/nvme0n4 detected for uuid a2eb3747-ae4b-4f62-ab66-6392b60865ea
2022-02-21 19:20:59.835 7 INFO nova.virt.libvirt.driver [req-7923e58c-001e-4ded-a92e-4be904f15752 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 7480ab10-22ba-4886-a571-f77c60fda1bf] Creating image
2022-02-21 19:21:00.840 7 INFO nova.compute.manager [req-31c940fc-0e9c-45b8-9877-95dd7faa5f15 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 588650e2-0ecb-4684-bd71-53f95481f013] Detaching volume eed44304-c133-4140-a247-184f48b1a812
2022-02-21 19:21:00.912 7 INFO nova.virt.block_device [req-31c940fc-0e9c-45b8-9877-95dd7faa5f15 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 588650e2-0ecb-4684-bd71-53f95481f013] Attempting to driver detach volume eed44304-c133-4140-a247-184f48b1a812 from mountpoint /dev/vdb
2022-02-21 19:21:00.933 7 INFO nova.virt.libvirt.driver [req-31c940fc-0e9c-45b8-9877-95dd7faa5f15 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] Successfully detached device vdb from instance 588650e2-0ecb-4684-bd71-53f95481f013 from the persistent domain config.
2022-02-21 19:21:01.069 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 7480ab10-22ba-4886-a571-f77c60fda1bf] VM Resumed (Lifecycle Event)
2022-02-21 19:21:01.077 7 INFO nova.virt.libvirt.driver [-] [instance: 7480ab10-22ba-4886-a571-f77c60fda1bf] Instance spawned successfully.
2022-02-21 19:21:01.077 7 INFO nova.compute.manager [req-7923e58c-001e-4ded-a92e-4be904f15752 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 7480ab10-22ba-4886-a571-f77c60fda1bf] Took 1.24 seconds to spawn the instance on the hypervisor.
2022-02-21 19:21:01.123 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 7480ab10-22ba-4886-a571-f77c60fda1bf] During sync_power_state the instance has a pending task (spawning). Skip.
2022-02-21 19:21:01.124 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 7480ab10-22ba-4886-a571-f77c60fda1bf] VM Started (Lifecycle Event)
2022-02-21 19:21:01.129 7 INFO nova.virt.libvirt.driver [req-31c940fc-0e9c-45b8-9877-95dd7faa5f15 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] Successfully detached device vdb from instance 588650e2-0ecb-4684-bd71-53f95481f013 from the live domain config.
2022-02-21 19:21:01.163 7 INFO nova.compute.manager [req-7923e58c-001e-4ded-a92e-4be904f15752 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 7480ab10-22ba-4886-a571-f77c60fda1bf] Took 1.75 seconds to build instance.
2022-02-21 19:21:01.197 7 INFO nova.virt.libvirt.driver [req-31c940fc-0e9c-45b8-9877-95dd7faa5f15 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] Detected multiple connections on this host for volume: eed44304-c133-4140-a247-184f48b1a812, skipping target disconnect.
2022-02-21 19:21:03.314 7 INFO nova.compute.manager [req-9b08b348-39c0-4902-bf8f-a5b33266b084 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Detaching volume eed44304-c133-4140-a247-184f48b1a812
2022-02-21 19:21:03.367 7 INFO nova.virt.block_device [req-9b08b348-39c0-4902-bf8f-a5b33266b084 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Attempting to driver detach volume eed44304-c133-4140-a247-184f48b1a812 from mountpoint /dev/vdb
2022-02-21 19:21:03.389 7 INFO nova.virt.libvirt.driver [req-9b08b348-39c0-4902-bf8f-a5b33266b084 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] Successfully detached device vdb from instance c33e1858-a599-46c3-b928-d3ef59def2b7 from the persistent domain config.
2022-02-21 19:21:03.555 7 INFO nova.virt.libvirt.driver [req-9b08b348-39c0-4902-bf8f-a5b33266b084 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] Successfully detached device vdb from instance c33e1858-a599-46c3-b928-d3ef59def2b7 from the live domain config.
2022-02-21 19:21:03.624 7 INFO os_brick.initiator.connectors.lightos [req-9b08b348-39c0-4902-bf8f-a5b33266b084 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] LIGHTOS: connect_volume called for volume a2eb3747-ae4b-4f62-ab66-6392b60865ea, connection properties: {'lightos_nodes': {'[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}}, 'uuid': 'a2eb3747-ae4b-4f62-ab66-6392b60865ea', 'subsysnqn': 'nqn.2016-01.com.lightbitslabs:uuid:2df610f7-a728-4fa9-8ec0-3ecb55dad510', 'qos_specs': None, 'access_mode': 'rw', 'encrypted': False, 'cacheable': False, 'device_path': '/dev/nvme0n4'}
2022-02-21 19:21:03.973 7 INFO nova.compute.manager [req-7fa542b8-8790-4fdb-a1dc-b9e29793ddc1 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 7480ab10-22ba-4886-a571-f77c60fda1bf] Attaching volume cb711706-80a5-4da2-9c1b-f2cf0061c25e to /dev/vdb
2022-02-21 19:21:04.062 7 WARNING os_brick.initiator.connectors.nvmeof [req-7fa542b8-8790-4fdb-a1dc-b9e29793ddc1 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] Process execution error in _get_host_uuid: Unexpected error while running command.
Command: blkid overlay -s UUID -o value
Exit code: 2
Stdout: ''
Stderr: '': oslo_concurrency.processutils.ProcessExecutionError: Unexpected error while running command.
2022-02-21 19:22:04.142 7 ERROR nova.virt.block_device [req-9b08b348-39c0-4902-bf8f-a5b33266b084 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Failed to detach volume eed44304-c133-4140-a247-184f48b1a812 from /dev/vdb: os_brick.exception.BrickException: Device with uuid a2eb3747-ae4b-4f62-ab66-6392b60865ea did not show up
2022-02-21 19:22:04.142 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Traceback (most recent call last):
2022-02-21 19:22:04.142 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/block_device.py", line 314, in driver_detach
2022-02-21 19:22:04.142 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]     virt_driver.detach_volume(context, connection_info, instance, mp,
2022-02-21 19:22:04.142 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/libvirt/driver.py", line 2611, in detach_volume
2022-02-21 19:22:04.142 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]     self._disconnect_volume(context, connection_info, instance,
2022-02-21 19:22:04.142 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/libvirt/driver.py", line 1877, in _disconnect_volume
2022-02-21 19:22:04.142 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]     vol_driver.disconnect_volume(connection_info, instance)
2022-02-21 19:22:04.142 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/libvirt/volume/lightos.py", line 47, in disconnect_volume
2022-02-21 19:22:04.142 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]     device_info = self.connector.connect_volume(connection_info['data'])
2022-02-21 19:22:04.142 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]   File "/var/lib/kolla/venv/lib/python3.8/site-packages/os_brick/utils.py", line 164, in trace_logging_wrapper
2022-02-21 19:22:04.142 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]     return f(*args, **kwargs)
2022-02-21 19:22:04.142 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_concurrency/lockutils.py", line 391, in inner
2022-02-21 19:22:04.142 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]     return f(*args, **kwargs)
2022-02-21 19:22:04.142 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]   File "/var/lib/kolla/venv/lib/python3.8/site-packages/os_brick/initiator/connectors/lightos.py", line 283, in connect_volume
2022-02-21 19:22:04.142 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]     raise exception.BrickException(message=msg)
2022-02-21 19:22:04.142 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] os_brick.exception.BrickException: Device with uuid a2eb3747-ae4b-4f62-ab66-6392b60865ea did not show up
2022-02-21 19:22:04.142 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] 
2022-02-21 19:22:04.154 7 INFO os_brick.initiator.connectors.lightos [req-7fa542b8-8790-4fdb-a1dc-b9e29793ddc1 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] LIGHTOS: connect_volume called for volume f4477b04-cd6c-4195-924c-f8d880d39297, connection properties: {'lightos_nodes': {'[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}}, 'uuid': 'f4477b04-cd6c-4195-924c-f8d880d39297', 'subsysnqn': 'nqn.2016-01.com.lightbitslabs:uuid:2df610f7-a728-4fa9-8ec0-3ecb55dad510', 'qos_specs': None, 'access_mode': 'rw', 'encrypted': False, 'cacheable': False}
2022-02-21 19:22:04.157 7 INFO os_brick.initiator.connectors.lightos [req-7fa542b8-8790-4fdb-a1dc-b9e29793ddc1 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid f4477b04-cd6c-4195-924c-f8d880d39297
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server [req-9b08b348-39c0-4902-bf8f-a5b33266b084 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] Exception during message handling: os_brick.exception.BrickException: Device with uuid a2eb3747-ae4b-4f62-ab66-6392b60865ea did not show up
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server Traceback (most recent call last):
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_messaging/rpc/server.py", line 165, in _process_incoming
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     res = self.dispatcher.dispatch(message)
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_messaging/rpc/dispatcher.py", line 309, in dispatch
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     return self._do_dispatch(endpoint, method, ctxt, args)
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_messaging/rpc/dispatcher.py", line 229, in _do_dispatch
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     result = func(ctxt, **new_args)
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/exception_wrapper.py", line 71, in wrapped
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     _emit_versioned_exception_notification(
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_utils/excutils.py", line 227, in __exit__
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     self.force_reraise()
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_utils/excutils.py", line 200, in force_reraise
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     raise self.value
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/exception_wrapper.py", line 63, in wrapped
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     return f(self, context, *args, **kw)
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/compute/utils.py", line 1437, in decorated_function
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     return function(self, context, *args, **kwargs)
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/compute/manager.py", line 211, in decorated_function
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     compute_utils.add_instance_fault_from_exc(context,
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_utils/excutils.py", line 227, in __exit__
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     self.force_reraise()
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_utils/excutils.py", line 200, in force_reraise
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     raise self.value
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/compute/manager.py", line 200, in decorated_function
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     return function(self, context, *args, **kwargs)
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/compute/manager.py", line 7207, in detach_volume
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     do_detach_volume(context, volume_id, instance, attachment_id)
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_concurrency/lockutils.py", line 391, in inner
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     return f(*args, **kwargs)
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/compute/manager.py", line 7204, in do_detach_volume
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     self._detach_volume(context, bdm, instance,
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/compute/manager.py", line 7155, in _detach_volume
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     driver_bdm.detach(context, instance, self.volume_api, self.driver,
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/block_device.py", line 473, in detach
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     self._do_detach(context, instance, volume_api, virt_driver,
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/block_device.py", line 394, in _do_detach
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     self.driver_detach(context, instance, volume_api, virt_driver)
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/block_device.py", line 333, in driver_detach
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     volume_api.roll_detaching(context, volume_id)
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_utils/excutils.py", line 227, in __exit__
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     self.force_reraise()
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_utils/excutils.py", line 200, in force_reraise
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     raise self.value
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/block_device.py", line 314, in driver_detach
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     virt_driver.detach_volume(context, connection_info, instance, mp,
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/libvirt/driver.py", line 2611, in detach_volume
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     self._disconnect_volume(context, connection_info, instance,
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/libvirt/driver.py", line 1877, in _disconnect_volume
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     vol_driver.disconnect_volume(connection_info, instance)
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/libvirt/volume/lightos.py", line 47, in disconnect_volume
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     device_info = self.connector.connect_volume(connection_info['data'])
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/os_brick/utils.py", line 164, in trace_logging_wrapper
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     return f(*args, **kwargs)
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_concurrency/lockutils.py", line 391, in inner
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     return f(*args, **kwargs)
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/os_brick/initiator/connectors/lightos.py", line 283, in connect_volume
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server     raise exception.BrickException(message=msg)
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server os_brick.exception.BrickException: Device with uuid a2eb3747-ae4b-4f62-ab66-6392b60865ea did not show up
2022-02-21 19:22:04.249 7 ERROR oslo_messaging.rpc.server 
2022-02-21 19:22:05.809 7 INFO nova.compute.manager [req-bf8ef7f0-6e4e-4303-b492-851f17905a7c 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 7480ab10-22ba-4886-a571-f77c60fda1bf] Detaching volume cb711706-80a5-4da2-9c1b-f2cf0061c25e
2022-02-21 19:22:05.868 7 INFO nova.virt.block_device [req-bf8ef7f0-6e4e-4303-b492-851f17905a7c 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 7480ab10-22ba-4886-a571-f77c60fda1bf] Attempting to driver detach volume cb711706-80a5-4da2-9c1b-f2cf0061c25e from mountpoint /dev/vdb
2022-02-21 19:22:05.886 7 INFO nova.virt.libvirt.driver [req-bf8ef7f0-6e4e-4303-b492-851f17905a7c 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] Successfully detached device vdb from instance 7480ab10-22ba-4886-a571-f77c60fda1bf from the persistent domain config.
2022-02-21 19:22:06.038 7 INFO nova.virt.libvirt.driver [req-bf8ef7f0-6e4e-4303-b492-851f17905a7c 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] Successfully detached device vdb from instance 7480ab10-22ba-4886-a571-f77c60fda1bf from the live domain config.
2022-02-21 19:22:06.041 7 INFO os_brick.initiator.connectors.lightos [req-bf8ef7f0-6e4e-4303-b492-851f17905a7c 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] LIGHTOS: connect_volume called for volume f4477b04-cd6c-4195-924c-f8d880d39297, connection properties: {'lightos_nodes': {'[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}}, 'uuid': 'f4477b04-cd6c-4195-924c-f8d880d39297', 'subsysnqn': 'nqn.2016-01.com.lightbitslabs:uuid:2df610f7-a728-4fa9-8ec0-3ecb55dad510', 'qos_specs': None, 'access_mode': 'rw', 'encrypted': False, 'cacheable': False, 'device_path': '/dev/nvme0n1'}
2022-02-21 19:22:06.042 7 INFO os_brick.initiator.connectors.lightos [req-bf8ef7f0-6e4e-4303-b492-851f17905a7c 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid f4477b04-cd6c-4195-924c-f8d880d39297
2022-02-21 19:22:06.043 7 INFO os_brick.initiator.connectors.lightos [req-bf8ef7f0-6e4e-4303-b492-851f17905a7c 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid f4477b04-cd6c-4195-924c-f8d880d39297
2022-02-21 19:22:09.246 7 INFO nova.compute.manager [req-11359b3b-f2db-4540-9e74-3a9d1154dbfb 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 7480ab10-22ba-4886-a571-f77c60fda1bf] Terminating instance
2022-02-21 19:22:09.625 7 INFO nova.virt.libvirt.driver [-] [instance: 7480ab10-22ba-4886-a571-f77c60fda1bf] Instance destroyed successfully.
2022-02-21 19:22:09.640 7 INFO nova.virt.libvirt.driver [req-11359b3b-f2db-4540-9e74-3a9d1154dbfb 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 7480ab10-22ba-4886-a571-f77c60fda1bf] Deleting instance files /var/lib/nova/instances/7480ab10-22ba-4886-a571-f77c60fda1bf_del
2022-02-21 19:22:09.642 7 INFO nova.virt.libvirt.driver [req-11359b3b-f2db-4540-9e74-3a9d1154dbfb 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 7480ab10-22ba-4886-a571-f77c60fda1bf] Deletion of /var/lib/nova/instances/7480ab10-22ba-4886-a571-f77c60fda1bf_del complete
2022-02-21 19:22:09.703 7 INFO nova.virt.libvirt.host [req-11359b3b-f2db-4540-9e74-3a9d1154dbfb 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] UEFI support detected
2022-02-21 19:22:09.706 7 INFO nova.compute.manager [req-11359b3b-f2db-4540-9e74-3a9d1154dbfb 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 7480ab10-22ba-4886-a571-f77c60fda1bf] Took 0.30 seconds to destroy the instance on the hypervisor.
2022-02-21 19:22:09.776 7 INFO nova.compute.manager [-] [instance: 7480ab10-22ba-4886-a571-f77c60fda1bf] Took 0.07 seconds to deallocate network for instance.
2022-02-21 19:22:09.972 7 INFO nova.scheduler.client.report [req-11359b3b-f2db-4540-9e74-3a9d1154dbfb 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] Deleted allocations for instance 7480ab10-22ba-4886-a571-f77c60fda1bf
2022-02-21 19:22:11.637 7 INFO nova.compute.manager [req-6ddc50f5-5b35-465c-869a-13d29b5da60a 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 89b44f49-5340-47ce-b489-6ceafcad65e2] Terminating instance
2022-02-21 19:22:11.978 7 INFO nova.virt.libvirt.driver [-] [instance: 89b44f49-5340-47ce-b489-6ceafcad65e2] Instance destroyed successfully.
2022-02-21 19:22:11.992 7 INFO nova.virt.libvirt.driver [req-6ddc50f5-5b35-465c-869a-13d29b5da60a 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 89b44f49-5340-47ce-b489-6ceafcad65e2] Deleting instance files /var/lib/nova/instances/89b44f49-5340-47ce-b489-6ceafcad65e2_del
2022-02-21 19:22:11.994 7 INFO nova.virt.libvirt.driver [req-6ddc50f5-5b35-465c-869a-13d29b5da60a 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 89b44f49-5340-47ce-b489-6ceafcad65e2] Deletion of /var/lib/nova/instances/89b44f49-5340-47ce-b489-6ceafcad65e2_del complete
2022-02-21 19:22:12.061 7 INFO nova.compute.manager [req-6ddc50f5-5b35-465c-869a-13d29b5da60a 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 89b44f49-5340-47ce-b489-6ceafcad65e2] Took 0.30 seconds to destroy the instance on the hypervisor.
2022-02-21 19:22:12.139 7 INFO nova.compute.manager [-] [instance: 89b44f49-5340-47ce-b489-6ceafcad65e2] Took 0.08 seconds to deallocate network for instance.
2022-02-21 19:22:12.334 7 INFO nova.scheduler.client.report [req-6ddc50f5-5b35-465c-869a-13d29b5da60a 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] Deleted allocations for instance 89b44f49-5340-47ce-b489-6ceafcad65e2
2022-02-21 19:22:12.882 7 INFO nova.compute.manager [req-1d9f425d-bd32-43bd-bfef-5c3e136a67a4 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 104de1d5-97a2-47e6-ac83-48561650dd3f] Terminating instance
2022-02-21 19:22:13.236 7 INFO nova.virt.libvirt.driver [-] [instance: 104de1d5-97a2-47e6-ac83-48561650dd3f] Instance destroyed successfully.
2022-02-21 19:22:13.255 7 INFO nova.virt.libvirt.driver [req-1d9f425d-bd32-43bd-bfef-5c3e136a67a4 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 104de1d5-97a2-47e6-ac83-48561650dd3f] Deleting instance files /var/lib/nova/instances/104de1d5-97a2-47e6-ac83-48561650dd3f_del
2022-02-21 19:22:13.257 7 INFO nova.virt.libvirt.driver [req-1d9f425d-bd32-43bd-bfef-5c3e136a67a4 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 104de1d5-97a2-47e6-ac83-48561650dd3f] Deletion of /var/lib/nova/instances/104de1d5-97a2-47e6-ac83-48561650dd3f_del complete
2022-02-21 19:22:13.334 7 INFO nova.compute.manager [req-1d9f425d-bd32-43bd-bfef-5c3e136a67a4 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 104de1d5-97a2-47e6-ac83-48561650dd3f] Took 0.32 seconds to destroy the instance on the hypervisor.
2022-02-21 19:22:13.400 7 INFO nova.compute.manager [-] [instance: 104de1d5-97a2-47e6-ac83-48561650dd3f] Took 0.06 seconds to deallocate network for instance.
2022-02-21 19:22:13.609 7 INFO nova.scheduler.client.report [req-1d9f425d-bd32-43bd-bfef-5c3e136a67a4 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] Deleted allocations for instance 104de1d5-97a2-47e6-ac83-48561650dd3f
2022-02-21 19:22:15.252 7 INFO nova.compute.manager [req-00a4e7dd-cde1-41da-aa71-34ffcdc156e5 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 206edc67-4e73-4b9a-9188-42e86bef0c37] Terminating instance
2022-02-21 19:22:15.350 7 INFO nova.compute.claims [req-27a357b1-f4fd-49cd-9f50-8a88c34880b9 7adaeae181c7452b8f87018052a3794a c0cc70ef9a56425ea2f529138d70bc92 - default default] [instance: a9f4d834-b4c1-4e09-99e6-87977f163b08] Claim successful on node rack08-server63
2022-02-21 19:22:15.598 7 INFO nova.virt.libvirt.driver [-] [instance: 206edc67-4e73-4b9a-9188-42e86bef0c37] Instance destroyed successfully.
2022-02-21 19:22:15.613 7 INFO nova.virt.libvirt.driver [req-00a4e7dd-cde1-41da-aa71-34ffcdc156e5 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 206edc67-4e73-4b9a-9188-42e86bef0c37] Deleting instance files /var/lib/nova/instances/206edc67-4e73-4b9a-9188-42e86bef0c37_del
2022-02-21 19:22:15.615 7 INFO nova.virt.libvirt.driver [req-00a4e7dd-cde1-41da-aa71-34ffcdc156e5 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 206edc67-4e73-4b9a-9188-42e86bef0c37] Deletion of /var/lib/nova/instances/206edc67-4e73-4b9a-9188-42e86bef0c37_del complete
2022-02-21 19:22:15.696 7 INFO nova.compute.manager [req-00a4e7dd-cde1-41da-aa71-34ffcdc156e5 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] [instance: 206edc67-4e73-4b9a-9188-42e86bef0c37] Took 0.32 seconds to destroy the instance on the hypervisor.
2022-02-21 19:22:15.789 7 INFO nova.compute.manager [-] [instance: 206edc67-4e73-4b9a-9188-42e86bef0c37] Took 0.09 seconds to deallocate network for instance.
2022-02-21 19:22:15.840 7 INFO nova.virt.libvirt.driver [req-27a357b1-f4fd-49cd-9f50-8a88c34880b9 7adaeae181c7452b8f87018052a3794a c0cc70ef9a56425ea2f529138d70bc92 - default default] [instance: a9f4d834-b4c1-4e09-99e6-87977f163b08] Creating image
2022-02-21 19:22:15.992 7 INFO nova.scheduler.client.report [req-00a4e7dd-cde1-41da-aa71-34ffcdc156e5 35d4fc9b0b404783b25bb1c84167ee1f 33e55a5c01c14220a390a63c9a0a2b01 - default default] Deleted allocations for instance 206edc67-4e73-4b9a-9188-42e86bef0c37
2022-02-21 19:22:16.983 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: a9f4d834-b4c1-4e09-99e6-87977f163b08] VM Resumed (Lifecycle Event)
2022-02-21 19:22:16.991 7 INFO nova.virt.libvirt.driver [-] [instance: a9f4d834-b4c1-4e09-99e6-87977f163b08] Instance spawned successfully.
2022-02-21 19:22:16.991 7 INFO nova.compute.manager [req-27a357b1-f4fd-49cd-9f50-8a88c34880b9 7adaeae181c7452b8f87018052a3794a c0cc70ef9a56425ea2f529138d70bc92 - default default] [instance: a9f4d834-b4c1-4e09-99e6-87977f163b08] Took 1.15 seconds to spawn the instance on the hypervisor.
2022-02-21 19:22:17.043 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: a9f4d834-b4c1-4e09-99e6-87977f163b08] During sync_power_state the instance has a pending task (spawning). Skip.
2022-02-21 19:22:17.044 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: a9f4d834-b4c1-4e09-99e6-87977f163b08] VM Started (Lifecycle Event)
2022-02-21 19:22:17.078 7 INFO nova.compute.manager [req-27a357b1-f4fd-49cd-9f50-8a88c34880b9 7adaeae181c7452b8f87018052a3794a c0cc70ef9a56425ea2f529138d70bc92 - default default] [instance: a9f4d834-b4c1-4e09-99e6-87977f163b08] Took 1.77 seconds to build instance.
2022-02-21 19:22:20.827 7 INFO nova.compute.manager [req-68ee4423-73f8-4f8d-8311-b3d0e46b3284 7adaeae181c7452b8f87018052a3794a c0cc70ef9a56425ea2f529138d70bc92 - default default] [instance: a9f4d834-b4c1-4e09-99e6-87977f163b08] Terminating instance
2022-02-21 19:22:21.182 7 INFO nova.virt.libvirt.driver [-] [instance: a9f4d834-b4c1-4e09-99e6-87977f163b08] Instance destroyed successfully.
2022-02-21 19:22:21.197 7 INFO nova.virt.libvirt.driver [req-68ee4423-73f8-4f8d-8311-b3d0e46b3284 7adaeae181c7452b8f87018052a3794a c0cc70ef9a56425ea2f529138d70bc92 - default default] [instance: a9f4d834-b4c1-4e09-99e6-87977f163b08] Deleting instance files /var/lib/nova/instances/a9f4d834-b4c1-4e09-99e6-87977f163b08_del
2022-02-21 19:22:21.198 7 INFO nova.virt.libvirt.driver [req-68ee4423-73f8-4f8d-8311-b3d0e46b3284 7adaeae181c7452b8f87018052a3794a c0cc70ef9a56425ea2f529138d70bc92 - default default] [instance: a9f4d834-b4c1-4e09-99e6-87977f163b08] Deletion of /var/lib/nova/instances/a9f4d834-b4c1-4e09-99e6-87977f163b08_del complete
2022-02-21 19:22:21.278 7 INFO nova.compute.manager [req-68ee4423-73f8-4f8d-8311-b3d0e46b3284 7adaeae181c7452b8f87018052a3794a c0cc70ef9a56425ea2f529138d70bc92 - default default] [instance: a9f4d834-b4c1-4e09-99e6-87977f163b08] Took 0.32 seconds to destroy the instance on the hypervisor.
2022-02-21 19:22:21.345 7 INFO nova.compute.manager [-] [instance: a9f4d834-b4c1-4e09-99e6-87977f163b08] Took 0.07 seconds to deallocate network for instance.
2022-02-21 19:22:21.546 7 INFO nova.scheduler.client.report [req-68ee4423-73f8-4f8d-8311-b3d0e46b3284 7adaeae181c7452b8f87018052a3794a c0cc70ef9a56425ea2f529138d70bc92 - default default] Deleted allocations for instance a9f4d834-b4c1-4e09-99e6-87977f163b08
2022-02-21 19:22:24.623 7 INFO nova.compute.manager [-] [instance: 7480ab10-22ba-4886-a571-f77c60fda1bf] VM Stopped (Lifecycle Event)
2022-02-21 19:22:26.975 7 INFO nova.compute.manager [-] [instance: 89b44f49-5340-47ce-b489-6ceafcad65e2] VM Stopped (Lifecycle Event)
2022-02-21 19:22:28.233 7 INFO nova.compute.manager [-] [instance: 104de1d5-97a2-47e6-ac83-48561650dd3f] VM Stopped (Lifecycle Event)
2022-02-21 19:22:30.596 7 INFO nova.compute.manager [-] [instance: 206edc67-4e73-4b9a-9188-42e86bef0c37] VM Stopped (Lifecycle Event)
2022-02-21 19:22:36.179 7 INFO nova.compute.manager [-] [instance: a9f4d834-b4c1-4e09-99e6-87977f163b08] VM Stopped (Lifecycle Event)
2022-02-21 19:23:49.760 7 INFO nova.compute.claims [req-4cb5f211-fd34-4d41-9873-828692f14840 bdc93e882d9a42b7895f2b4bc33ac57f d97bacd4fb384d5ca1b72504d241b68e - default default] [instance: d567f6e6-b23a-450e-a381-3bd055532326] Claim successful on node rack08-server63
2022-02-21 19:23:50.155 7 INFO nova.virt.libvirt.driver [req-4cb5f211-fd34-4d41-9873-828692f14840 bdc93e882d9a42b7895f2b4bc33ac57f d97bacd4fb384d5ca1b72504d241b68e - default default] [instance: d567f6e6-b23a-450e-a381-3bd055532326] Creating image
2022-02-21 19:23:51.363 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: d567f6e6-b23a-450e-a381-3bd055532326] VM Resumed (Lifecycle Event)
2022-02-21 19:23:51.371 7 INFO nova.virt.libvirt.driver [-] [instance: d567f6e6-b23a-450e-a381-3bd055532326] Instance spawned successfully.
2022-02-21 19:23:51.372 7 INFO nova.compute.manager [req-4cb5f211-fd34-4d41-9873-828692f14840 bdc93e882d9a42b7895f2b4bc33ac57f d97bacd4fb384d5ca1b72504d241b68e - default default] [instance: d567f6e6-b23a-450e-a381-3bd055532326] Took 1.22 seconds to spawn the instance on the hypervisor.
2022-02-21 19:23:51.422 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: d567f6e6-b23a-450e-a381-3bd055532326] During sync_power_state the instance has a pending task (spawning). Skip.
2022-02-21 19:23:51.423 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: d567f6e6-b23a-450e-a381-3bd055532326] VM Started (Lifecycle Event)
2022-02-21 19:23:51.464 7 INFO nova.compute.manager [req-4cb5f211-fd34-4d41-9873-828692f14840 bdc93e882d9a42b7895f2b4bc33ac57f d97bacd4fb384d5ca1b72504d241b68e - default default] [instance: d567f6e6-b23a-450e-a381-3bd055532326] Took 1.74 seconds to build instance.
2022-02-21 19:23:51.770 7 INFO nova.virt.libvirt.driver [req-8c252673-c732-4f7c-ad6b-dbf5aeec4122 bdc93e882d9a42b7895f2b4bc33ac57f d97bacd4fb384d5ca1b72504d241b68e - default default] [instance: d567f6e6-b23a-450e-a381-3bd055532326] Ignoring supplied device name: /dev/vdb
2022-02-21 19:23:51.947 7 INFO nova.compute.manager [req-8c252673-c732-4f7c-ad6b-dbf5aeec4122 bdc93e882d9a42b7895f2b4bc33ac57f d97bacd4fb384d5ca1b72504d241b68e - default default] [instance: d567f6e6-b23a-450e-a381-3bd055532326] Attaching volume cb5a3407-8831-44ac-89b4-7eb6f8cadca3 to /dev/vdb
2022-02-21 19:23:52.045 7 WARNING os_brick.initiator.connectors.nvmeof [req-8c252673-c732-4f7c-ad6b-dbf5aeec4122 bdc93e882d9a42b7895f2b4bc33ac57f d97bacd4fb384d5ca1b72504d241b68e - default default] Process execution error in _get_host_uuid: Unexpected error while running command.
Command: blkid overlay -s UUID -o value
Exit code: 2
Stdout: ''
Stderr: '': oslo_concurrency.processutils.ProcessExecutionError: Unexpected error while running command.
2022-02-21 19:23:53.393 7 INFO os_brick.initiator.connectors.lightos [req-8c252673-c732-4f7c-ad6b-dbf5aeec4122 bdc93e882d9a42b7895f2b4bc33ac57f d97bacd4fb384d5ca1b72504d241b68e - default default] LIGHTOS: connect_volume called for volume 824d0245-40ca-4531-9c1c-a44fa7afe191, connection properties: {'lightos_nodes': {'[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}}, 'uuid': '824d0245-40ca-4531-9c1c-a44fa7afe191', 'subsysnqn': 'nqn.2016-01.com.lightbitslabs:uuid:2df610f7-a728-4fa9-8ec0-3ecb55dad510', 'qos_specs': None, 'access_mode': 'rw', 'encrypted': False, 'cacheable': False}
2022-02-21 19:23:53.397 7 INFO os_brick.initiator.connectors.lightos [req-8c252673-c732-4f7c-ad6b-dbf5aeec4122 bdc93e882d9a42b7895f2b4bc33ac57f d97bacd4fb384d5ca1b72504d241b68e - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid 824d0245-40ca-4531-9c1c-a44fa7afe191
2022-02-21 19:23:55.172 7 INFO nova.compute.manager [req-d3d6efa6-139d-4891-b619-df61b7194ea6 e12fe8b009bc475e8267d14a0328dbf9 a0863d78b77f4cdd844f9510e7ad8692 - default default] [instance: d567f6e6-b23a-450e-a381-3bd055532326] Cinder extended volume cb5a3407-8831-44ac-89b4-7eb6f8cadca3; extending it to detect new size
2022-02-21 19:23:55.241 7 INFO os_brick.initiator.connectors.lightos [req-d3d6efa6-139d-4891-b619-df61b7194ea6 e12fe8b009bc475e8267d14a0328dbf9 a0863d78b77f4cdd844f9510e7ad8692 - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid 824d0245-40ca-4531-9c1c-a44fa7afe191
2022-02-21 19:23:55.930 7 INFO nova.compute.manager [req-34790b41-9241-4af4-8c00-3343ae4a0373 bdc93e882d9a42b7895f2b4bc33ac57f d97bacd4fb384d5ca1b72504d241b68e - default default] [instance: d567f6e6-b23a-450e-a381-3bd055532326] Detaching volume cb5a3407-8831-44ac-89b4-7eb6f8cadca3
2022-02-21 19:23:55.986 7 INFO nova.virt.block_device [req-34790b41-9241-4af4-8c00-3343ae4a0373 bdc93e882d9a42b7895f2b4bc33ac57f d97bacd4fb384d5ca1b72504d241b68e - default default] [instance: d567f6e6-b23a-450e-a381-3bd055532326] Attempting to driver detach volume cb5a3407-8831-44ac-89b4-7eb6f8cadca3 from mountpoint /dev/vdb
2022-02-21 19:23:56.004 7 INFO nova.virt.libvirt.driver [req-34790b41-9241-4af4-8c00-3343ae4a0373 bdc93e882d9a42b7895f2b4bc33ac57f d97bacd4fb384d5ca1b72504d241b68e - default default] Successfully detached device vdb from instance d567f6e6-b23a-450e-a381-3bd055532326 from the persistent domain config.
2022-02-21 19:23:56.146 7 INFO nova.virt.libvirt.driver [req-34790b41-9241-4af4-8c00-3343ae4a0373 bdc93e882d9a42b7895f2b4bc33ac57f d97bacd4fb384d5ca1b72504d241b68e - default default] Successfully detached device vdb from instance d567f6e6-b23a-450e-a381-3bd055532326 from the live domain config.
2022-02-21 19:23:56.149 7 INFO os_brick.initiator.connectors.lightos [req-34790b41-9241-4af4-8c00-3343ae4a0373 bdc93e882d9a42b7895f2b4bc33ac57f d97bacd4fb384d5ca1b72504d241b68e - default default] LIGHTOS: connect_volume called for volume 824d0245-40ca-4531-9c1c-a44fa7afe191, connection properties: {'lightos_nodes': {'[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}}, 'uuid': '824d0245-40ca-4531-9c1c-a44fa7afe191', 'subsysnqn': 'nqn.2016-01.com.lightbitslabs:uuid:2df610f7-a728-4fa9-8ec0-3ecb55dad510', 'qos_specs': None, 'access_mode': 'rw', 'encrypted': False, 'cacheable': False, 'device_path': '/dev/nvme0n1'}
2022-02-21 19:23:56.150 7 INFO os_brick.initiator.connectors.lightos [req-34790b41-9241-4af4-8c00-3343ae4a0373 bdc93e882d9a42b7895f2b4bc33ac57f d97bacd4fb384d5ca1b72504d241b68e - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid 824d0245-40ca-4531-9c1c-a44fa7afe191
2022-02-21 19:23:56.151 7 INFO os_brick.initiator.connectors.lightos [req-34790b41-9241-4af4-8c00-3343ae4a0373 bdc93e882d9a42b7895f2b4bc33ac57f d97bacd4fb384d5ca1b72504d241b68e - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid 824d0245-40ca-4531-9c1c-a44fa7afe191
2022-02-21 19:23:58.173 7 INFO nova.compute.manager [req-8c17edc1-ac9f-4310-b4b5-65007b5f7985 bdc93e882d9a42b7895f2b4bc33ac57f d97bacd4fb384d5ca1b72504d241b68e - default default] [instance: d567f6e6-b23a-450e-a381-3bd055532326] Terminating instance
2022-02-21 19:23:58.536 7 INFO nova.virt.libvirt.driver [-] [instance: d567f6e6-b23a-450e-a381-3bd055532326] Instance destroyed successfully.
2022-02-21 19:23:58.552 7 INFO nova.virt.libvirt.driver [req-8c17edc1-ac9f-4310-b4b5-65007b5f7985 bdc93e882d9a42b7895f2b4bc33ac57f d97bacd4fb384d5ca1b72504d241b68e - default default] [instance: d567f6e6-b23a-450e-a381-3bd055532326] Deleting instance files /var/lib/nova/instances/d567f6e6-b23a-450e-a381-3bd055532326_del
2022-02-21 19:23:58.553 7 INFO nova.virt.libvirt.driver [req-8c17edc1-ac9f-4310-b4b5-65007b5f7985 bdc93e882d9a42b7895f2b4bc33ac57f d97bacd4fb384d5ca1b72504d241b68e - default default] [instance: d567f6e6-b23a-450e-a381-3bd055532326] Deletion of /var/lib/nova/instances/d567f6e6-b23a-450e-a381-3bd055532326_del complete
2022-02-21 19:23:58.621 7 INFO nova.compute.manager [req-8c17edc1-ac9f-4310-b4b5-65007b5f7985 bdc93e882d9a42b7895f2b4bc33ac57f d97bacd4fb384d5ca1b72504d241b68e - default default] [instance: d567f6e6-b23a-450e-a381-3bd055532326] Took 0.31 seconds to destroy the instance on the hypervisor.
2022-02-21 19:23:58.688 7 INFO nova.compute.manager [-] [instance: d567f6e6-b23a-450e-a381-3bd055532326] Took 0.07 seconds to deallocate network for instance.
2022-02-21 19:23:58.881 7 INFO nova.scheduler.client.report [req-8c17edc1-ac9f-4310-b4b5-65007b5f7985 bdc93e882d9a42b7895f2b4bc33ac57f d97bacd4fb384d5ca1b72504d241b68e - default default] Deleted allocations for instance d567f6e6-b23a-450e-a381-3bd055532326
2022-02-21 19:24:13.534 7 INFO nova.compute.manager [-] [instance: d567f6e6-b23a-450e-a381-3bd055532326] VM Stopped (Lifecycle Event)
2022-02-21 19:25:29.261 7 INFO nova.compute.claims [req-525e06b4-5b9a-4f6a-878c-ef0f717f9d73 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: dc46f8d8-2e5a-464b-a5c8-72b979547485] Claim successful on node rack08-server63
2022-02-21 19:25:29.663 7 INFO nova.virt.libvirt.driver [req-525e06b4-5b9a-4f6a-878c-ef0f717f9d73 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: dc46f8d8-2e5a-464b-a5c8-72b979547485] Creating image
2022-02-21 19:25:30.817 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: dc46f8d8-2e5a-464b-a5c8-72b979547485] VM Resumed (Lifecycle Event)
2022-02-21 19:25:30.824 7 INFO nova.virt.libvirt.driver [-] [instance: dc46f8d8-2e5a-464b-a5c8-72b979547485] Instance spawned successfully.
2022-02-21 19:25:30.825 7 INFO nova.compute.manager [req-525e06b4-5b9a-4f6a-878c-ef0f717f9d73 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: dc46f8d8-2e5a-464b-a5c8-72b979547485] Took 1.16 seconds to spawn the instance on the hypervisor.
2022-02-21 19:25:30.871 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: dc46f8d8-2e5a-464b-a5c8-72b979547485] During sync_power_state the instance has a pending task (spawning). Skip.
2022-02-21 19:25:30.872 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: dc46f8d8-2e5a-464b-a5c8-72b979547485] VM Started (Lifecycle Event)
2022-02-21 19:25:30.911 7 INFO nova.compute.manager [req-525e06b4-5b9a-4f6a-878c-ef0f717f9d73 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: dc46f8d8-2e5a-464b-a5c8-72b979547485] Took 1.69 seconds to build instance.
2022-02-21 19:25:46.921 7 INFO nova.virt.libvirt.driver [req-1f2b3d31-4ca3-4726-8874-8b88fea404c3 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: dc46f8d8-2e5a-464b-a5c8-72b979547485] Ignoring supplied device name: /dev/vdb
2022-02-21 19:25:47.085 7 INFO nova.compute.manager [req-1f2b3d31-4ca3-4726-8874-8b88fea404c3 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: dc46f8d8-2e5a-464b-a5c8-72b979547485] Attaching volume b3b8390e-c7e8-44d9-bf93-0578bf74fce3 to /dev/vdb
2022-02-21 19:25:47.179 7 WARNING os_brick.initiator.connectors.nvmeof [req-1f2b3d31-4ca3-4726-8874-8b88fea404c3 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] Process execution error in _get_host_uuid: Unexpected error while running command.
Command: blkid overlay -s UUID -o value
Exit code: 2
Stdout: ''
Stderr: '': oslo_concurrency.processutils.ProcessExecutionError: Unexpected error while running command.
2022-02-21 19:25:48.525 7 INFO os_brick.initiator.connectors.lightos [req-1f2b3d31-4ca3-4726-8874-8b88fea404c3 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] LIGHTOS: connect_volume called for volume 2979d58b-f53f-49bc-8ecc-cb7498d55af3, connection properties: {'lightos_nodes': {'[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}}, 'uuid': '2979d58b-f53f-49bc-8ecc-cb7498d55af3', 'subsysnqn': 'nqn.2016-01.com.lightbitslabs:uuid:2df610f7-a728-4fa9-8ec0-3ecb55dad510', 'qos_specs': None, 'access_mode': 'rw', 'encrypted': False, 'cacheable': False}
2022-02-21 19:25:48.529 7 INFO os_brick.initiator.connectors.lightos [req-1f2b3d31-4ca3-4726-8874-8b88fea404c3 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid 2979d58b-f53f-49bc-8ecc-cb7498d55af3
2022-02-21 19:25:59.663 7 INFO nova.compute.manager [req-74ef8948-e0ba-4087-8b4b-fe17de491d46 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: dc46f8d8-2e5a-464b-a5c8-72b979547485] Detaching volume b3b8390e-c7e8-44d9-bf93-0578bf74fce3
2022-02-21 19:25:59.724 7 INFO nova.virt.block_device [req-74ef8948-e0ba-4087-8b4b-fe17de491d46 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: dc46f8d8-2e5a-464b-a5c8-72b979547485] Attempting to driver detach volume b3b8390e-c7e8-44d9-bf93-0578bf74fce3 from mountpoint /dev/vdb
2022-02-21 19:25:59.744 7 INFO nova.virt.libvirt.driver [req-74ef8948-e0ba-4087-8b4b-fe17de491d46 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] Successfully detached device vdb from instance dc46f8d8-2e5a-464b-a5c8-72b979547485 from the persistent domain config.
2022-02-21 19:25:59.895 7 INFO nova.virt.libvirt.driver [req-74ef8948-e0ba-4087-8b4b-fe17de491d46 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] Successfully detached device vdb from instance dc46f8d8-2e5a-464b-a5c8-72b979547485 from the live domain config.
2022-02-21 19:25:59.898 7 INFO os_brick.initiator.connectors.lightos [req-74ef8948-e0ba-4087-8b4b-fe17de491d46 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] LIGHTOS: connect_volume called for volume 2979d58b-f53f-49bc-8ecc-cb7498d55af3, connection properties: {'lightos_nodes': {'[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}}, 'uuid': '2979d58b-f53f-49bc-8ecc-cb7498d55af3', 'subsysnqn': 'nqn.2016-01.com.lightbitslabs:uuid:2df610f7-a728-4fa9-8ec0-3ecb55dad510', 'qos_specs': None, 'access_mode': 'rw', 'encrypted': False, 'cacheable': False, 'device_path': '/dev/nvme0n1'}
2022-02-21 19:25:59.899 7 INFO os_brick.initiator.connectors.lightos [req-74ef8948-e0ba-4087-8b4b-fe17de491d46 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid 2979d58b-f53f-49bc-8ecc-cb7498d55af3
2022-02-21 19:25:59.900 7 INFO os_brick.initiator.connectors.lightos [req-74ef8948-e0ba-4087-8b4b-fe17de491d46 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid 2979d58b-f53f-49bc-8ecc-cb7498d55af3
2022-02-21 19:26:01.944 7 INFO nova.compute.manager [req-cc2988e2-5856-4d5c-99f5-9be1da622e99 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: dc46f8d8-2e5a-464b-a5c8-72b979547485] Terminating instance
2022-02-21 19:26:02.292 7 INFO nova.virt.libvirt.driver [-] [instance: dc46f8d8-2e5a-464b-a5c8-72b979547485] Instance destroyed successfully.
2022-02-21 19:26:02.306 7 INFO nova.virt.libvirt.driver [req-cc2988e2-5856-4d5c-99f5-9be1da622e99 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: dc46f8d8-2e5a-464b-a5c8-72b979547485] Deleting instance files /var/lib/nova/instances/dc46f8d8-2e5a-464b-a5c8-72b979547485_del
2022-02-21 19:26:02.307 7 INFO nova.virt.libvirt.driver [req-cc2988e2-5856-4d5c-99f5-9be1da622e99 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: dc46f8d8-2e5a-464b-a5c8-72b979547485] Deletion of /var/lib/nova/instances/dc46f8d8-2e5a-464b-a5c8-72b979547485_del complete
2022-02-21 19:26:02.377 7 INFO nova.compute.manager [req-cc2988e2-5856-4d5c-99f5-9be1da622e99 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: dc46f8d8-2e5a-464b-a5c8-72b979547485] Took 0.31 seconds to destroy the instance on the hypervisor.
2022-02-21 19:26:02.447 7 INFO nova.compute.manager [-] [instance: dc46f8d8-2e5a-464b-a5c8-72b979547485] Took 0.07 seconds to deallocate network for instance.
2022-02-21 19:26:02.643 7 INFO nova.scheduler.client.report [req-cc2988e2-5856-4d5c-99f5-9be1da622e99 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] Deleted allocations for instance dc46f8d8-2e5a-464b-a5c8-72b979547485
2022-02-21 19:26:03.578 7 INFO nova.compute.manager [req-d04b6b7f-e328-4054-9e0f-e05dd1269998 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Detaching volume eed44304-c133-4140-a247-184f48b1a812
2022-02-21 19:26:03.638 7 INFO nova.virt.block_device [req-d04b6b7f-e328-4054-9e0f-e05dd1269998 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Attempting to driver detach volume eed44304-c133-4140-a247-184f48b1a812 from mountpoint /dev/vdb
2022-02-21 19:26:03.649 7 INFO nova.virt.libvirt.driver [req-d04b6b7f-e328-4054-9e0f-e05dd1269998 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Device vdb not found in instance.
2022-02-21 19:26:03.700 7 INFO os_brick.initiator.connectors.lightos [req-d04b6b7f-e328-4054-9e0f-e05dd1269998 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] LIGHTOS: connect_volume called for volume a2eb3747-ae4b-4f62-ab66-6392b60865ea, connection properties: {'lightos_nodes': {'[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}}, 'uuid': 'a2eb3747-ae4b-4f62-ab66-6392b60865ea', 'subsysnqn': 'nqn.2016-01.com.lightbitslabs:uuid:2df610f7-a728-4fa9-8ec0-3ecb55dad510', 'qos_specs': None, 'access_mode': 'rw', 'encrypted': False, 'cacheable': False, 'device_path': '/dev/nvme0n4'}
2022-02-21 19:26:10.364 7 INFO nova.compute.claims [req-ec2ee4cf-9436-464f-96ba-672c2f34fdb1 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: 6903c720-367b-4712-92fe-296f217ce199] Claim successful on node rack08-server63
2022-02-21 19:26:10.765 7 INFO nova.virt.libvirt.driver [req-ec2ee4cf-9436-464f-96ba-672c2f34fdb1 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: 6903c720-367b-4712-92fe-296f217ce199] Creating image
2022-02-21 19:26:11.900 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 6903c720-367b-4712-92fe-296f217ce199] VM Resumed (Lifecycle Event)
2022-02-21 19:26:11.908 7 INFO nova.virt.libvirt.driver [-] [instance: 6903c720-367b-4712-92fe-296f217ce199] Instance spawned successfully.
2022-02-21 19:26:11.909 7 INFO nova.compute.manager [req-ec2ee4cf-9436-464f-96ba-672c2f34fdb1 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: 6903c720-367b-4712-92fe-296f217ce199] Took 1.14 seconds to spawn the instance on the hypervisor.
2022-02-21 19:26:11.957 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 6903c720-367b-4712-92fe-296f217ce199] During sync_power_state the instance has a pending task (spawning). Skip.
2022-02-21 19:26:11.958 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 6903c720-367b-4712-92fe-296f217ce199] VM Started (Lifecycle Event)
2022-02-21 19:26:11.998 7 INFO nova.compute.manager [req-ec2ee4cf-9436-464f-96ba-672c2f34fdb1 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: 6903c720-367b-4712-92fe-296f217ce199] Took 1.67 seconds to build instance.
2022-02-21 19:26:12.216 7 INFO nova.virt.libvirt.driver [req-215a5bd7-5d47-43a5-864b-6b5a3460b037 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: 6903c720-367b-4712-92fe-296f217ce199] Ignoring supplied device name: /dev/vdb
2022-02-21 19:26:12.383 7 INFO nova.compute.manager [req-215a5bd7-5d47-43a5-864b-6b5a3460b037 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: 6903c720-367b-4712-92fe-296f217ce199] Attaching volume dd78ae92-0607-4d48-9934-99136aad65b1 to /dev/vdb
2022-02-21 19:26:12.465 7 WARNING os_brick.initiator.connectors.nvmeof [req-215a5bd7-5d47-43a5-864b-6b5a3460b037 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] Process execution error in _get_host_uuid: Unexpected error while running command.
Command: blkid overlay -s UUID -o value
Exit code: 2
Stdout: ''
Stderr: '': oslo_concurrency.processutils.ProcessExecutionError: Unexpected error while running command.
2022-02-21 19:26:16.047 7 INFO nova.compute.claims [req-157517f4-b11b-4dcf-be31-3f3785e6cda6 a2cc4189cc5f4e1eb5cd6409e239d103 07057fca3a0c4a64a5492e6daa3c3b77 - default default] [instance: 418fd14e-42eb-4bee-b006-3552590975d0] Claim successful on node rack08-server63
2022-02-21 19:26:16.438 7 INFO nova.virt.libvirt.driver [req-157517f4-b11b-4dcf-be31-3f3785e6cda6 a2cc4189cc5f4e1eb5cd6409e239d103 07057fca3a0c4a64a5492e6daa3c3b77 - default default] [instance: 418fd14e-42eb-4bee-b006-3552590975d0] Creating image
2022-02-21 19:26:17.289 7 INFO nova.compute.manager [-] [instance: dc46f8d8-2e5a-464b-a5c8-72b979547485] VM Stopped (Lifecycle Event)
2022-02-21 19:26:17.591 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 418fd14e-42eb-4bee-b006-3552590975d0] VM Resumed (Lifecycle Event)
2022-02-21 19:26:17.598 7 INFO nova.virt.libvirt.driver [-] [instance: 418fd14e-42eb-4bee-b006-3552590975d0] Instance spawned successfully.
2022-02-21 19:26:17.599 7 INFO nova.compute.manager [req-157517f4-b11b-4dcf-be31-3f3785e6cda6 a2cc4189cc5f4e1eb5cd6409e239d103 07057fca3a0c4a64a5492e6daa3c3b77 - default default] [instance: 418fd14e-42eb-4bee-b006-3552590975d0] Took 1.16 seconds to spawn the instance on the hypervisor.
2022-02-21 19:26:17.649 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 418fd14e-42eb-4bee-b006-3552590975d0] During sync_power_state the instance has a pending task (spawning). Skip.
2022-02-21 19:26:17.649 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: 418fd14e-42eb-4bee-b006-3552590975d0] VM Started (Lifecycle Event)
2022-02-21 19:26:17.686 7 INFO nova.compute.manager [req-157517f4-b11b-4dcf-be31-3f3785e6cda6 a2cc4189cc5f4e1eb5cd6409e239d103 07057fca3a0c4a64a5492e6daa3c3b77 - default default] [instance: 418fd14e-42eb-4bee-b006-3552590975d0] Took 1.68 seconds to build instance.
2022-02-21 19:26:19.466 7 INFO nova.compute.manager [req-a83e0434-ef47-46f2-8ae7-1dc65d307951 a2cc4189cc5f4e1eb5cd6409e239d103 07057fca3a0c4a64a5492e6daa3c3b77 - default default] [instance: 418fd14e-42eb-4bee-b006-3552590975d0] Terminating instance
2022-02-21 19:26:19.822 7 INFO nova.virt.libvirt.driver [-] [instance: 418fd14e-42eb-4bee-b006-3552590975d0] Instance destroyed successfully.
2022-02-21 19:26:19.837 7 INFO nova.virt.libvirt.driver [req-a83e0434-ef47-46f2-8ae7-1dc65d307951 a2cc4189cc5f4e1eb5cd6409e239d103 07057fca3a0c4a64a5492e6daa3c3b77 - default default] [instance: 418fd14e-42eb-4bee-b006-3552590975d0] Deleting instance files /var/lib/nova/instances/418fd14e-42eb-4bee-b006-3552590975d0_del
2022-02-21 19:26:19.838 7 INFO nova.virt.libvirt.driver [req-a83e0434-ef47-46f2-8ae7-1dc65d307951 a2cc4189cc5f4e1eb5cd6409e239d103 07057fca3a0c4a64a5492e6daa3c3b77 - default default] [instance: 418fd14e-42eb-4bee-b006-3552590975d0] Deletion of /var/lib/nova/instances/418fd14e-42eb-4bee-b006-3552590975d0_del complete
2022-02-21 19:26:19.910 7 INFO nova.compute.manager [req-a83e0434-ef47-46f2-8ae7-1dc65d307951 a2cc4189cc5f4e1eb5cd6409e239d103 07057fca3a0c4a64a5492e6daa3c3b77 - default default] [instance: 418fd14e-42eb-4bee-b006-3552590975d0] Took 0.31 seconds to destroy the instance on the hypervisor.
2022-02-21 19:26:19.976 7 INFO nova.compute.manager [-] [instance: 418fd14e-42eb-4bee-b006-3552590975d0] Took 0.07 seconds to deallocate network for instance.
2022-02-21 19:26:20.173 7 INFO nova.scheduler.client.report [req-a83e0434-ef47-46f2-8ae7-1dc65d307951 a2cc4189cc5f4e1eb5cd6409e239d103 07057fca3a0c4a64a5492e6daa3c3b77 - default default] Deleted allocations for instance 418fd14e-42eb-4bee-b006-3552590975d0
2022-02-21 19:26:21.939 7 INFO nova.compute.claims [req-770a753d-fb4c-4f79-9689-c0744c1fcbf9 a2cc4189cc5f4e1eb5cd6409e239d103 07057fca3a0c4a64a5492e6daa3c3b77 - default default] [instance: bdfeb6f3-19cf-4aa5-a02c-eee87b8a19ed] Claim successful on node rack08-server63
2022-02-21 19:26:22.340 7 INFO nova.virt.libvirt.driver [req-770a753d-fb4c-4f79-9689-c0744c1fcbf9 a2cc4189cc5f4e1eb5cd6409e239d103 07057fca3a0c4a64a5492e6daa3c3b77 - default default] [instance: bdfeb6f3-19cf-4aa5-a02c-eee87b8a19ed] Creating image
2022-02-21 19:26:23.486 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: bdfeb6f3-19cf-4aa5-a02c-eee87b8a19ed] VM Resumed (Lifecycle Event)
2022-02-21 19:26:23.493 7 INFO nova.virt.libvirt.driver [-] [instance: bdfeb6f3-19cf-4aa5-a02c-eee87b8a19ed] Instance spawned successfully.
2022-02-21 19:26:23.494 7 INFO nova.compute.manager [req-770a753d-fb4c-4f79-9689-c0744c1fcbf9 a2cc4189cc5f4e1eb5cd6409e239d103 07057fca3a0c4a64a5492e6daa3c3b77 - default default] [instance: bdfeb6f3-19cf-4aa5-a02c-eee87b8a19ed] Took 1.15 seconds to spawn the instance on the hypervisor.
2022-02-21 19:26:23.561 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: bdfeb6f3-19cf-4aa5-a02c-eee87b8a19ed] During sync_power_state the instance has a pending task (spawning). Skip.
2022-02-21 19:26:23.562 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: bdfeb6f3-19cf-4aa5-a02c-eee87b8a19ed] VM Started (Lifecycle Event)
2022-02-21 19:26:23.580 7 INFO nova.compute.manager [req-770a753d-fb4c-4f79-9689-c0744c1fcbf9 a2cc4189cc5f4e1eb5cd6409e239d103 07057fca3a0c4a64a5492e6daa3c3b77 - default default] [instance: bdfeb6f3-19cf-4aa5-a02c-eee87b8a19ed] Took 1.68 seconds to build instance.
2022-02-21 19:26:24.144 7 INFO nova.compute.manager [req-6e7f9e47-7c83-4bd6-92ce-ec2b9b24824a a2cc4189cc5f4e1eb5cd6409e239d103 07057fca3a0c4a64a5492e6daa3c3b77 - default default] [instance: bdfeb6f3-19cf-4aa5-a02c-eee87b8a19ed] Terminating instance
2022-02-21 19:26:24.506 7 INFO nova.virt.libvirt.driver [-] [instance: bdfeb6f3-19cf-4aa5-a02c-eee87b8a19ed] Instance destroyed successfully.
2022-02-21 19:26:24.525 7 INFO nova.virt.libvirt.driver [req-6e7f9e47-7c83-4bd6-92ce-ec2b9b24824a a2cc4189cc5f4e1eb5cd6409e239d103 07057fca3a0c4a64a5492e6daa3c3b77 - default default] [instance: bdfeb6f3-19cf-4aa5-a02c-eee87b8a19ed] Deleting instance files /var/lib/nova/instances/bdfeb6f3-19cf-4aa5-a02c-eee87b8a19ed_del
2022-02-21 19:26:24.526 7 INFO nova.virt.libvirt.driver [req-6e7f9e47-7c83-4bd6-92ce-ec2b9b24824a a2cc4189cc5f4e1eb5cd6409e239d103 07057fca3a0c4a64a5492e6daa3c3b77 - default default] [instance: bdfeb6f3-19cf-4aa5-a02c-eee87b8a19ed] Deletion of /var/lib/nova/instances/bdfeb6f3-19cf-4aa5-a02c-eee87b8a19ed_del complete
2022-02-21 19:26:24.608 7 INFO nova.compute.manager [req-6e7f9e47-7c83-4bd6-92ce-ec2b9b24824a a2cc4189cc5f4e1eb5cd6409e239d103 07057fca3a0c4a64a5492e6daa3c3b77 - default default] [instance: bdfeb6f3-19cf-4aa5-a02c-eee87b8a19ed] Took 0.32 seconds to destroy the instance on the hypervisor.
2022-02-21 19:26:24.674 7 INFO nova.compute.manager [-] [instance: bdfeb6f3-19cf-4aa5-a02c-eee87b8a19ed] Took 0.06 seconds to deallocate network for instance.
2022-02-21 19:26:24.843 7 INFO nova.scheduler.client.report [req-6e7f9e47-7c83-4bd6-92ce-ec2b9b24824a a2cc4189cc5f4e1eb5cd6409e239d103 07057fca3a0c4a64a5492e6daa3c3b77 - default default] Deleted allocations for instance bdfeb6f3-19cf-4aa5-a02c-eee87b8a19ed
2022-02-21 19:26:34.820 7 INFO nova.compute.manager [-] [instance: 418fd14e-42eb-4bee-b006-3552590975d0] VM Stopped (Lifecycle Event)
2022-02-21 19:26:39.503 7 INFO nova.compute.manager [-] [instance: bdfeb6f3-19cf-4aa5-a02c-eee87b8a19ed] VM Stopped (Lifecycle Event)
2022-02-21 19:27:04.351 7 ERROR nova.virt.block_device [req-d04b6b7f-e328-4054-9e0f-e05dd1269998 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Failed to detach volume eed44304-c133-4140-a247-184f48b1a812 from /dev/vdb: os_brick.exception.BrickException: Device with uuid a2eb3747-ae4b-4f62-ab66-6392b60865ea did not show up
2022-02-21 19:27:04.351 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Traceback (most recent call last):
2022-02-21 19:27:04.351 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/block_device.py", line 314, in driver_detach
2022-02-21 19:27:04.351 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]     virt_driver.detach_volume(context, connection_info, instance, mp,
2022-02-21 19:27:04.351 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/libvirt/driver.py", line 2611, in detach_volume
2022-02-21 19:27:04.351 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]     self._disconnect_volume(context, connection_info, instance,
2022-02-21 19:27:04.351 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/libvirt/driver.py", line 1877, in _disconnect_volume
2022-02-21 19:27:04.351 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]     vol_driver.disconnect_volume(connection_info, instance)
2022-02-21 19:27:04.351 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/libvirt/volume/lightos.py", line 47, in disconnect_volume
2022-02-21 19:27:04.351 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]     device_info = self.connector.connect_volume(connection_info['data'])
2022-02-21 19:27:04.351 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]   File "/var/lib/kolla/venv/lib/python3.8/site-packages/os_brick/utils.py", line 164, in trace_logging_wrapper
2022-02-21 19:27:04.351 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]     return f(*args, **kwargs)
2022-02-21 19:27:04.351 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_concurrency/lockutils.py", line 391, in inner
2022-02-21 19:27:04.351 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]     return f(*args, **kwargs)
2022-02-21 19:27:04.351 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]   File "/var/lib/kolla/venv/lib/python3.8/site-packages/os_brick/initiator/connectors/lightos.py", line 283, in connect_volume
2022-02-21 19:27:04.351 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7]     raise exception.BrickException(message=msg)
2022-02-21 19:27:04.351 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] os_brick.exception.BrickException: Device with uuid a2eb3747-ae4b-4f62-ab66-6392b60865ea did not show up
2022-02-21 19:27:04.351 7 ERROR nova.virt.block_device [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] 
2022-02-21 19:27:04.357 7 INFO os_brick.initiator.connectors.lightos [req-215a5bd7-5d47-43a5-864b-6b5a3460b037 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] LIGHTOS: connect_volume called for volume 7c370740-9c41-4f0d-b78b-666c431b0fc7, connection properties: {'lightos_nodes': {'[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}}, 'uuid': '7c370740-9c41-4f0d-b78b-666c431b0fc7', 'subsysnqn': 'nqn.2016-01.com.lightbitslabs:uuid:2df610f7-a728-4fa9-8ec0-3ecb55dad510', 'qos_specs': None, 'access_mode': 'rw', 'encrypted': False, 'cacheable': False}
2022-02-21 19:27:04.360 7 INFO os_brick.initiator.connectors.lightos [req-215a5bd7-5d47-43a5-864b-6b5a3460b037 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid 7c370740-9c41-4f0d-b78b-666c431b0fc7
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server [req-d04b6b7f-e328-4054-9e0f-e05dd1269998 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] Exception during message handling: os_brick.exception.BrickException: Device with uuid a2eb3747-ae4b-4f62-ab66-6392b60865ea did not show up
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server Traceback (most recent call last):
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_messaging/rpc/server.py", line 165, in _process_incoming
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     res = self.dispatcher.dispatch(message)
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_messaging/rpc/dispatcher.py", line 309, in dispatch
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     return self._do_dispatch(endpoint, method, ctxt, args)
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_messaging/rpc/dispatcher.py", line 229, in _do_dispatch
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     result = func(ctxt, **new_args)
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/exception_wrapper.py", line 71, in wrapped
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     _emit_versioned_exception_notification(
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_utils/excutils.py", line 227, in __exit__
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     self.force_reraise()
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_utils/excutils.py", line 200, in force_reraise
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     raise self.value
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/exception_wrapper.py", line 63, in wrapped
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     return f(self, context, *args, **kw)
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/compute/utils.py", line 1437, in decorated_function
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     return function(self, context, *args, **kwargs)
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/compute/manager.py", line 211, in decorated_function
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     compute_utils.add_instance_fault_from_exc(context,
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_utils/excutils.py", line 227, in __exit__
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     self.force_reraise()
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_utils/excutils.py", line 200, in force_reraise
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     raise self.value
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/compute/manager.py", line 200, in decorated_function
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     return function(self, context, *args, **kwargs)
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/compute/manager.py", line 7207, in detach_volume
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     do_detach_volume(context, volume_id, instance, attachment_id)
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_concurrency/lockutils.py", line 391, in inner
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     return f(*args, **kwargs)
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/compute/manager.py", line 7204, in do_detach_volume
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     self._detach_volume(context, bdm, instance,
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/compute/manager.py", line 7155, in _detach_volume
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     driver_bdm.detach(context, instance, self.volume_api, self.driver,
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/block_device.py", line 473, in detach
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     self._do_detach(context, instance, volume_api, virt_driver,
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/block_device.py", line 394, in _do_detach
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     self.driver_detach(context, instance, volume_api, virt_driver)
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/block_device.py", line 333, in driver_detach
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     volume_api.roll_detaching(context, volume_id)
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_utils/excutils.py", line 227, in __exit__
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     self.force_reraise()
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_utils/excutils.py", line 200, in force_reraise
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     raise self.value
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/block_device.py", line 314, in driver_detach
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     virt_driver.detach_volume(context, connection_info, instance, mp,
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/libvirt/driver.py", line 2611, in detach_volume
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     self._disconnect_volume(context, connection_info, instance,
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/libvirt/driver.py", line 1877, in _disconnect_volume
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     vol_driver.disconnect_volume(connection_info, instance)
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/nova/virt/libvirt/volume/lightos.py", line 47, in disconnect_volume
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     device_info = self.connector.connect_volume(connection_info['data'])
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/os_brick/utils.py", line 164, in trace_logging_wrapper
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     return f(*args, **kwargs)
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/oslo_concurrency/lockutils.py", line 391, in inner
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     return f(*args, **kwargs)
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server   File "/var/lib/kolla/venv/lib/python3.8/site-packages/os_brick/initiator/connectors/lightos.py", line 283, in connect_volume
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server     raise exception.BrickException(message=msg)
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server os_brick.exception.BrickException: Device with uuid a2eb3747-ae4b-4f62-ab66-6392b60865ea did not show up
2022-02-21 19:27:04.446 7 ERROR oslo_messaging.rpc.server 
2022-02-21 19:27:13.568 7 INFO nova.compute.manager [req-6f67a42c-afe3-411b-a240-33e8acb3df86 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: 6903c720-367b-4712-92fe-296f217ce199] Detaching volume dd78ae92-0607-4d48-9934-99136aad65b1
2022-02-21 19:27:13.958 7 INFO nova.virt.block_device [req-6f67a42c-afe3-411b-a240-33e8acb3df86 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: 6903c720-367b-4712-92fe-296f217ce199] Attempting to driver detach volume dd78ae92-0607-4d48-9934-99136aad65b1 from mountpoint /dev/vdb
2022-02-21 19:27:13.981 7 INFO nova.virt.libvirt.driver [req-6f67a42c-afe3-411b-a240-33e8acb3df86 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] Successfully detached device vdb from instance 6903c720-367b-4712-92fe-296f217ce199 from the persistent domain config.
2022-02-21 19:27:14.131 7 INFO nova.virt.libvirt.driver [req-6f67a42c-afe3-411b-a240-33e8acb3df86 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] Successfully detached device vdb from instance 6903c720-367b-4712-92fe-296f217ce199 from the live domain config.
2022-02-21 19:27:14.134 7 INFO os_brick.initiator.connectors.lightos [req-6f67a42c-afe3-411b-a240-33e8acb3df86 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] LIGHTOS: connect_volume called for volume 7c370740-9c41-4f0d-b78b-666c431b0fc7, connection properties: {'lightos_nodes': {'[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}}, 'uuid': '7c370740-9c41-4f0d-b78b-666c431b0fc7', 'subsysnqn': 'nqn.2016-01.com.lightbitslabs:uuid:2df610f7-a728-4fa9-8ec0-3ecb55dad510', 'qos_specs': None, 'access_mode': 'rw', 'encrypted': False, 'cacheable': False, 'device_path': '/dev/nvme0n1'}
2022-02-21 19:27:14.135 7 INFO os_brick.initiator.connectors.lightos [req-6f67a42c-afe3-411b-a240-33e8acb3df86 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid 7c370740-9c41-4f0d-b78b-666c431b0fc7
2022-02-21 19:27:14.136 7 INFO os_brick.initiator.connectors.lightos [req-6f67a42c-afe3-411b-a240-33e8acb3df86 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid 7c370740-9c41-4f0d-b78b-666c431b0fc7
2022-02-21 19:27:15.817 7 INFO nova.compute.manager [req-6c1df019-9e92-4a78-b0b0-ee32712c1a85 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: 6903c720-367b-4712-92fe-296f217ce199] Terminating instance
2022-02-21 19:27:16.201 7 INFO nova.virt.libvirt.driver [-] [instance: 6903c720-367b-4712-92fe-296f217ce199] Instance destroyed successfully.
2022-02-21 19:27:16.216 7 INFO nova.virt.libvirt.driver [req-6c1df019-9e92-4a78-b0b0-ee32712c1a85 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: 6903c720-367b-4712-92fe-296f217ce199] Deleting instance files /var/lib/nova/instances/6903c720-367b-4712-92fe-296f217ce199_del
2022-02-21 19:27:16.218 7 INFO nova.virt.libvirt.driver [req-6c1df019-9e92-4a78-b0b0-ee32712c1a85 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: 6903c720-367b-4712-92fe-296f217ce199] Deletion of /var/lib/nova/instances/6903c720-367b-4712-92fe-296f217ce199_del complete
2022-02-21 19:27:16.285 7 INFO nova.compute.manager [req-6c1df019-9e92-4a78-b0b0-ee32712c1a85 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] [instance: 6903c720-367b-4712-92fe-296f217ce199] Took 0.31 seconds to destroy the instance on the hypervisor.
2022-02-21 19:27:16.346 7 INFO nova.compute.manager [-] [instance: 6903c720-367b-4712-92fe-296f217ce199] Took 0.06 seconds to deallocate network for instance.
2022-02-21 19:27:16.520 7 INFO nova.scheduler.client.report [req-6c1df019-9e92-4a78-b0b0-ee32712c1a85 c88507a4add74837a90ee48cae42fe7a fd545dcb1b0946a5b49092fe4469e32f - default default] Deleted allocations for instance 6903c720-367b-4712-92fe-296f217ce199
2022-02-21 19:27:21.630 7 INFO nova.compute.claims [req-bafc3c77-93b6-4aac-8099-2635b2a9118f 9ff4c5bf2bc440c39070177b12aa50b6 1ed9a5f47e114c36a5718c1bf4a3d8d8 - default default] [instance: e5121af7-9bdd-4a48-afd7-c65aea267b84] Claim successful on node rack08-server63
2022-02-21 19:27:22.032 7 INFO nova.virt.libvirt.driver [req-bafc3c77-93b6-4aac-8099-2635b2a9118f 9ff4c5bf2bc440c39070177b12aa50b6 1ed9a5f47e114c36a5718c1bf4a3d8d8 - default default] [instance: e5121af7-9bdd-4a48-afd7-c65aea267b84] Creating image
2022-02-21 19:27:23.186 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: e5121af7-9bdd-4a48-afd7-c65aea267b84] VM Resumed (Lifecycle Event)
2022-02-21 19:27:23.194 7 INFO nova.virt.libvirt.driver [-] [instance: e5121af7-9bdd-4a48-afd7-c65aea267b84] Instance spawned successfully.
2022-02-21 19:27:23.195 7 INFO nova.compute.manager [req-bafc3c77-93b6-4aac-8099-2635b2a9118f 9ff4c5bf2bc440c39070177b12aa50b6 1ed9a5f47e114c36a5718c1bf4a3d8d8 - default default] [instance: e5121af7-9bdd-4a48-afd7-c65aea267b84] Took 1.16 seconds to spawn the instance on the hypervisor.
2022-02-21 19:27:23.244 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: e5121af7-9bdd-4a48-afd7-c65aea267b84] During sync_power_state the instance has a pending task (spawning). Skip.
2022-02-21 19:27:23.245 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: e5121af7-9bdd-4a48-afd7-c65aea267b84] VM Started (Lifecycle Event)
2022-02-21 19:27:23.300 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: e5121af7-9bdd-4a48-afd7-c65aea267b84] During sync_power_state the instance has a pending task (spawning). Skip.
2022-02-21 19:27:23.312 7 INFO nova.compute.manager [req-bafc3c77-93b6-4aac-8099-2635b2a9118f 9ff4c5bf2bc440c39070177b12aa50b6 1ed9a5f47e114c36a5718c1bf4a3d8d8 - default default] [instance: e5121af7-9bdd-4a48-afd7-c65aea267b84] Took 1.72 seconds to build instance.
2022-02-21 19:27:24.583 7 INFO nova.compute.manager [req-4b17e607-3d1d-47b4-9a54-22ce3b092370 9ff4c5bf2bc440c39070177b12aa50b6 1ed9a5f47e114c36a5718c1bf4a3d8d8 - default default] [instance: e5121af7-9bdd-4a48-afd7-c65aea267b84] Terminating instance
2022-02-21 19:27:24.923 7 INFO nova.virt.libvirt.driver [-] [instance: e5121af7-9bdd-4a48-afd7-c65aea267b84] Instance destroyed successfully.
2022-02-21 19:27:24.937 7 INFO nova.virt.libvirt.driver [req-4b17e607-3d1d-47b4-9a54-22ce3b092370 9ff4c5bf2bc440c39070177b12aa50b6 1ed9a5f47e114c36a5718c1bf4a3d8d8 - default default] [instance: e5121af7-9bdd-4a48-afd7-c65aea267b84] Deleting instance files /var/lib/nova/instances/e5121af7-9bdd-4a48-afd7-c65aea267b84_del
2022-02-21 19:27:24.938 7 INFO nova.virt.libvirt.driver [req-4b17e607-3d1d-47b4-9a54-22ce3b092370 9ff4c5bf2bc440c39070177b12aa50b6 1ed9a5f47e114c36a5718c1bf4a3d8d8 - default default] [instance: e5121af7-9bdd-4a48-afd7-c65aea267b84] Deletion of /var/lib/nova/instances/e5121af7-9bdd-4a48-afd7-c65aea267b84_del complete
2022-02-21 19:27:25.014 7 INFO nova.compute.manager [req-4b17e607-3d1d-47b4-9a54-22ce3b092370 9ff4c5bf2bc440c39070177b12aa50b6 1ed9a5f47e114c36a5718c1bf4a3d8d8 - default default] [instance: e5121af7-9bdd-4a48-afd7-c65aea267b84] Took 0.31 seconds to destroy the instance on the hypervisor.
2022-02-21 19:27:25.083 7 INFO nova.compute.manager [-] [instance: e5121af7-9bdd-4a48-afd7-c65aea267b84] Took 0.07 seconds to deallocate network for instance.
2022-02-21 19:27:25.276 7 INFO nova.scheduler.client.report [req-4b17e607-3d1d-47b4-9a54-22ce3b092370 9ff4c5bf2bc440c39070177b12aa50b6 1ed9a5f47e114c36a5718c1bf4a3d8d8 - default default] Deleted allocations for instance e5121af7-9bdd-4a48-afd7-c65aea267b84
2022-02-21 19:27:31.199 7 INFO nova.compute.manager [-] [instance: 6903c720-367b-4712-92fe-296f217ce199] VM Stopped (Lifecycle Event)
2022-02-21 19:27:39.921 7 INFO nova.compute.manager [-] [instance: e5121af7-9bdd-4a48-afd7-c65aea267b84] VM Stopped (Lifecycle Event)
2022-02-21 19:31:03.960 7 INFO nova.compute.manager [req-aebd8183-d576-49f7-8636-85f2d1519f40 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Get console output
2022-02-21 19:31:04.243 80 INFO nova.privsep.libvirt [-] Ignored error while reading from instance console pty: can't concat NoneType to bytes
2022-02-21 19:31:21.462 7 INFO nova.compute.claims [req-e0f36294-32d9-4b40-8985-868d6c65c833 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: fa6f6bad-9528-4e53-be1c-e5240a082e8a] Claim successful on node rack08-server63
2022-02-21 19:31:21.727 7 INFO nova.virt.libvirt.driver [req-e0f36294-32d9-4b40-8985-868d6c65c833 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: fa6f6bad-9528-4e53-be1c-e5240a082e8a] Ignoring supplied device name: /dev/vda. Libvirt can't honour user-supplied dev names
2022-02-21 19:31:21.817 7 INFO nova.virt.block_device [req-e0f36294-32d9-4b40-8985-868d6c65c833 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: fa6f6bad-9528-4e53-be1c-e5240a082e8a] Booting with volume 21c62a27-4a37-4547-9238-cca53435d1c1 at /dev/vda
2022-02-21 19:31:21.909 7 WARNING os_brick.initiator.connectors.nvmeof [req-e0f36294-32d9-4b40-8985-868d6c65c833 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] Process execution error in _get_host_uuid: Unexpected error while running command.
Command: blkid overlay -s UUID -o value
Exit code: 2
Stdout: ''
Stderr: '': oslo_concurrency.processutils.ProcessExecutionError: Unexpected error while running command.
2022-02-21 19:31:23.435 7 INFO nova.virt.libvirt.driver [req-e0f36294-32d9-4b40-8985-868d6c65c833 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: fa6f6bad-9528-4e53-be1c-e5240a082e8a] Creating image
2022-02-21 19:31:23.446 7 INFO os_brick.initiator.connectors.lightos [req-e0f36294-32d9-4b40-8985-868d6c65c833 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] LIGHTOS: connect_volume called for volume fa520c11-c783-4ff7-9b0f-a7009ad4decb, connection properties: {'lightos_nodes': {'[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}}, 'uuid': 'fa520c11-c783-4ff7-9b0f-a7009ad4decb', 'subsysnqn': 'nqn.2016-01.com.lightbitslabs:uuid:2df610f7-a728-4fa9-8ec0-3ecb55dad510', 'qos_specs': None, 'access_mode': 'rw', 'encrypted': False, 'cacheable': False}
2022-02-21 19:31:23.448 7 INFO os_brick.initiator.connectors.lightos [req-e0f36294-32d9-4b40-8985-868d6c65c833 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid fa520c11-c783-4ff7-9b0f-a7009ad4decb
2022-02-21 19:31:24.261 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: fa6f6bad-9528-4e53-be1c-e5240a082e8a] VM Resumed (Lifecycle Event)
2022-02-21 19:31:24.269 7 INFO nova.virt.libvirt.driver [-] [instance: fa6f6bad-9528-4e53-be1c-e5240a082e8a] Instance spawned successfully.
2022-02-21 19:31:24.269 7 INFO nova.compute.manager [req-e0f36294-32d9-4b40-8985-868d6c65c833 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: fa6f6bad-9528-4e53-be1c-e5240a082e8a] Took 0.84 seconds to spawn the instance on the hypervisor.
2022-02-21 19:31:24.318 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: fa6f6bad-9528-4e53-be1c-e5240a082e8a] During sync_power_state the instance has a pending task (spawning). Skip.
2022-02-21 19:31:24.319 7 INFO nova.compute.manager [req-d9056d0c-a895-474b-87d2-3c6fa982b428 - - - - -] [instance: fa6f6bad-9528-4e53-be1c-e5240a082e8a] VM Started (Lifecycle Event)
2022-02-21 19:31:24.357 7 INFO nova.compute.manager [req-e0f36294-32d9-4b40-8985-868d6c65c833 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: fa6f6bad-9528-4e53-be1c-e5240a082e8a] Took 2.94 seconds to build instance.
2022-02-21 19:31:25.768 7 INFO nova.compute.manager [req-06c9b286-f08f-416a-a131-0a67dd9302f9 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: fa6f6bad-9528-4e53-be1c-e5240a082e8a] Terminating instance
2022-02-21 19:31:26.116 7 INFO nova.virt.libvirt.driver [-] [instance: fa6f6bad-9528-4e53-be1c-e5240a082e8a] Instance destroyed successfully.
2022-02-21 19:31:26.186 7 INFO os_brick.initiator.connectors.lightos [req-06c9b286-f08f-416a-a131-0a67dd9302f9 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] LIGHTOS: connect_volume called for volume fa520c11-c783-4ff7-9b0f-a7009ad4decb, connection properties: {'lightos_nodes': {'[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}}, 'uuid': 'fa520c11-c783-4ff7-9b0f-a7009ad4decb', 'subsysnqn': 'nqn.2016-01.com.lightbitslabs:uuid:2df610f7-a728-4fa9-8ec0-3ecb55dad510', 'qos_specs': None, 'access_mode': 'rw', 'encrypted': False, 'cacheable': False, 'device_path': '/dev/nvme0n1'}
2022-02-21 19:31:26.187 7 INFO os_brick.initiator.connectors.lightos [req-06c9b286-f08f-416a-a131-0a67dd9302f9 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid fa520c11-c783-4ff7-9b0f-a7009ad4decb
2022-02-21 19:31:26.188 7 INFO os_brick.initiator.connectors.lightos [req-06c9b286-f08f-416a-a131-0a67dd9302f9 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] LIGHTOS: devpath /dev/nvme0n1 detected for uuid fa520c11-c783-4ff7-9b0f-a7009ad4decb
2022-02-21 19:31:26.199 7 INFO nova.virt.libvirt.driver [req-06c9b286-f08f-416a-a131-0a67dd9302f9 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: fa6f6bad-9528-4e53-be1c-e5240a082e8a] Deleting instance files /var/lib/nova/instances/fa6f6bad-9528-4e53-be1c-e5240a082e8a_del
2022-02-21 19:31:26.200 7 INFO nova.virt.libvirt.driver [req-06c9b286-f08f-416a-a131-0a67dd9302f9 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: fa6f6bad-9528-4e53-be1c-e5240a082e8a] Deletion of /var/lib/nova/instances/fa6f6bad-9528-4e53-be1c-e5240a082e8a_del complete
2022-02-21 19:31:26.273 7 INFO nova.compute.manager [req-06c9b286-f08f-416a-a131-0a67dd9302f9 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: fa6f6bad-9528-4e53-be1c-e5240a082e8a] Took 0.37 seconds to destroy the instance on the hypervisor.
2022-02-21 19:31:26.340 7 INFO nova.compute.manager [-] [instance: fa6f6bad-9528-4e53-be1c-e5240a082e8a] Took 0.07 seconds to deallocate network for instance.
2022-02-21 19:31:29.639 7 INFO nova.compute.manager [req-06c9b286-f08f-416a-a131-0a67dd9302f9 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: fa6f6bad-9528-4e53-be1c-e5240a082e8a] Took 3.30 seconds to detach 1 volumes for instance.
2022-02-21 19:31:29.835 7 INFO nova.scheduler.client.report [req-06c9b286-f08f-416a-a131-0a67dd9302f9 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] Deleted allocations for instance fa6f6bad-9528-4e53-be1c-e5240a082e8a
2022-02-21 19:31:41.114 7 INFO nova.compute.manager [-] [instance: fa6f6bad-9528-4e53-be1c-e5240a082e8a] VM Stopped (Lifecycle Event)
2022-02-21 19:36:31.265 7 INFO nova.compute.manager [req-7a6713e3-7233-417b-90a5-19934dd9b6fa 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Terminating instance
2022-02-21 19:36:31.638 7 INFO nova.virt.libvirt.driver [-] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Instance destroyed successfully.
2022-02-21 19:36:31.709 7 INFO os_brick.initiator.connectors.lightos [req-7a6713e3-7233-417b-90a5-19934dd9b6fa 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] LIGHTOS: connect_volume called for volume a2eb3747-ae4b-4f62-ab66-6392b60865ea, connection properties: {'lightos_nodes': {'[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}}, 'uuid': 'a2eb3747-ae4b-4f62-ab66-6392b60865ea', 'subsysnqn': 'nqn.2016-01.com.lightbitslabs:uuid:2df610f7-a728-4fa9-8ec0-3ecb55dad510', 'qos_specs': None, 'access_mode': 'rw', 'encrypted': False, 'cacheable': False, 'device_path': '/dev/nvme0n4'}
2022-02-21 19:36:46.637 7 INFO nova.compute.manager [-] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] VM Stopped (Lifecycle Event)
2022-02-21 19:36:46.696 7 INFO nova.compute.manager [req-655d3924-638c-44fb-b5e4-71fe0f44147d - - - - -] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] During sync_power_state the instance has a pending task (deleting). Skip.
2022-02-21 19:37:32.331 7 WARNING nova.virt.libvirt.driver [req-7a6713e3-7233-417b-90a5-19934dd9b6fa 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Ignoring Volume Error on vol eed44304-c133-4140-a247-184f48b1a812 during delete Device with uuid a2eb3747-ae4b-4f62-ab66-6392b60865ea did not show up: os_brick.exception.BrickException: Device with uuid a2eb3747-ae4b-4f62-ab66-6392b60865ea did not show up
2022-02-21 19:37:32.333 7 INFO nova.virt.libvirt.driver [req-7a6713e3-7233-417b-90a5-19934dd9b6fa 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Deleting instance files /var/lib/nova/instances/c33e1858-a599-46c3-b928-d3ef59def2b7_del
2022-02-21 19:37:32.335 7 INFO nova.virt.libvirt.driver [req-7a6713e3-7233-417b-90a5-19934dd9b6fa 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Deletion of /var/lib/nova/instances/c33e1858-a599-46c3-b928-d3ef59def2b7_del complete
2022-02-21 19:37:32.404 7 INFO nova.compute.manager [req-7a6713e3-7233-417b-90a5-19934dd9b6fa 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Took 60.99 seconds to destroy the instance on the hypervisor.
2022-02-21 19:37:32.472 7 INFO nova.compute.manager [-] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Took 0.07 seconds to deallocate network for instance.
2022-02-21 19:37:33.729 7 INFO nova.compute.manager [req-7a6713e3-7233-417b-90a5-19934dd9b6fa 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: c33e1858-a599-46c3-b928-d3ef59def2b7] Took 1.26 seconds to detach 1 volumes for instance.
2022-02-21 19:37:33.901 7 INFO nova.scheduler.client.report [req-7a6713e3-7233-417b-90a5-19934dd9b6fa 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] Deleted allocations for instance c33e1858-a599-46c3-b928-d3ef59def2b7
2022-02-21 19:37:34.635 7 INFO nova.compute.manager [req-3faac2e6-f3aa-4fb0-81db-1b6aa84423b4 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 588650e2-0ecb-4684-bd71-53f95481f013] Terminating instance
2022-02-21 19:37:34.997 7 INFO nova.virt.libvirt.driver [-] [instance: 588650e2-0ecb-4684-bd71-53f95481f013] Instance destroyed successfully.
2022-02-21 19:37:35.014 7 INFO nova.virt.libvirt.driver [req-3faac2e6-f3aa-4fb0-81db-1b6aa84423b4 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 588650e2-0ecb-4684-bd71-53f95481f013] Deleting instance files /var/lib/nova/instances/588650e2-0ecb-4684-bd71-53f95481f013_del
2022-02-21 19:37:35.016 7 INFO nova.virt.libvirt.driver [req-3faac2e6-f3aa-4fb0-81db-1b6aa84423b4 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 588650e2-0ecb-4684-bd71-53f95481f013] Deletion of /var/lib/nova/instances/588650e2-0ecb-4684-bd71-53f95481f013_del complete
2022-02-21 19:37:35.092 7 INFO nova.compute.manager [req-3faac2e6-f3aa-4fb0-81db-1b6aa84423b4 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 588650e2-0ecb-4684-bd71-53f95481f013] Took 0.32 seconds to destroy the instance on the hypervisor.
2022-02-21 19:37:35.155 7 INFO nova.compute.manager [-] [instance: 588650e2-0ecb-4684-bd71-53f95481f013] Took 0.06 seconds to deallocate network for instance.
2022-02-21 19:37:35.350 7 INFO nova.scheduler.client.report [req-3faac2e6-f3aa-4fb0-81db-1b6aa84423b4 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] Deleted allocations for instance 588650e2-0ecb-4684-bd71-53f95481f013
2022-02-21 19:37:35.895 7 INFO nova.compute.manager [req-f175e372-e3ce-4a76-899d-340990583526 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 6b416943-2b85-4dfa-b133-e9d5ec5d39fd] Terminating instance
2022-02-21 19:37:36.237 7 INFO nova.virt.libvirt.driver [-] [instance: 6b416943-2b85-4dfa-b133-e9d5ec5d39fd] Instance destroyed successfully.
2022-02-21 19:37:36.308 7 INFO os_brick.initiator.connectors.lightos [req-f175e372-e3ce-4a76-899d-340990583526 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] LIGHTOS: connect_volume called for volume c7aed01c-2118-4e01-9fa5-d25f08ec854e, connection properties: {'lightos_nodes': {'[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}, '[LB_IP_ADDRESS]': {'target_portal': '[LB_IP_ADDRESS]', 'target_port': 8009, 'transport_type': 'tcp'}}, 'uuid': 'c7aed01c-2118-4e01-9fa5-d25f08ec854e', 'subsysnqn': 'nqn.2016-01.com.lightbitslabs:uuid:2df610f7-a728-4fa9-8ec0-3ecb55dad510', 'qos_specs': None, 'access_mode': 'rw', 'encrypted': False, 'cacheable': False, 'device_path': '/dev/nvme0n3'}
2022-02-21 19:37:36.309 7 INFO os_brick.initiator.connectors.lightos [req-f175e372-e3ce-4a76-899d-340990583526 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] LIGHTOS: devpath /dev/nvme0n3 detected for uuid c7aed01c-2118-4e01-9fa5-d25f08ec854e
2022-02-21 19:37:36.310 7 INFO os_brick.initiator.connectors.lightos [req-f175e372-e3ce-4a76-899d-340990583526 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] LIGHTOS: devpath /dev/nvme0n3 detected for uuid c7aed01c-2118-4e01-9fa5-d25f08ec854e
2022-02-21 19:37:36.321 7 INFO nova.virt.libvirt.driver [req-f175e372-e3ce-4a76-899d-340990583526 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 6b416943-2b85-4dfa-b133-e9d5ec5d39fd] Deleting instance files /var/lib/nova/instances/6b416943-2b85-4dfa-b133-e9d5ec5d39fd_del
2022-02-21 19:37:36.323 7 INFO nova.virt.libvirt.driver [req-f175e372-e3ce-4a76-899d-340990583526 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 6b416943-2b85-4dfa-b133-e9d5ec5d39fd] Deletion of /var/lib/nova/instances/6b416943-2b85-4dfa-b133-e9d5ec5d39fd_del complete
2022-02-21 19:37:36.388 7 INFO nova.compute.manager [req-f175e372-e3ce-4a76-899d-340990583526 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 6b416943-2b85-4dfa-b133-e9d5ec5d39fd] Took 0.37 seconds to destroy the instance on the hypervisor.
2022-02-21 19:37:36.452 7 INFO nova.compute.manager [-] [instance: 6b416943-2b85-4dfa-b133-e9d5ec5d39fd] Took 0.06 seconds to deallocate network for instance.
2022-02-21 19:37:37.703 7 INFO nova.compute.manager [req-f175e372-e3ce-4a76-899d-340990583526 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] [instance: 6b416943-2b85-4dfa-b133-e9d5ec5d39fd] Took 1.25 seconds to detach 1 volumes for instance.
2022-02-21 19:37:37.894 7 INFO nova.scheduler.client.report [req-f175e372-e3ce-4a76-899d-340990583526 06a4077db90943c9b0040d607d59c990 bdd1a313d09440968d86ef28931df4ca - default default] Deleted allocations for instance 6b416943-2b85-4dfa-b133-e9d5ec5d39fd
2022-02-21 19:37:49.995 7 INFO nova.compute.manager [-] [instance: 588650e2-0ecb-4684-bd71-53f95481f013] VM Stopped (Lifecycle Event)
2022-02-21 19:37:51.235 7 INFO nova.compute.manager [-] [instance: 6b416943-2b85-4dfa-b133-e9d5ec5d39fd] VM Stopped (Lifecycle Event)
